{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93c8a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "from typing import Optional, List\n",
    "\n",
    "import scanpy as sc \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler, BatchSampler\n",
    "from torchvision.transforms import ColorJitter, Normalize, RandomHorizontalFlip, RandomVerticalFlip, RandomAdjustSharpness\n",
    "from timm import create_model\n",
    "from einops import rearrange, reduce\n",
    "from skimage.color import label2rgb\n",
    "from skimage.measure import regionprops_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e8c58b",
   "metadata": {},
   "source": [
    "\n",
    "note that you need to make bug fix to diffusers v0.3.0\n",
    "\n",
    "in ~/.local/lib/python3.9/site-packages/diffusers/models/unet_blocks.py you need to change out_channels parameter in DownEncoderBlock2D to make the unet work for >2 downsamples\n",
    "\n",
    "out_channels=in_channels if add_downsample else out_channels,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc8aa08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mestorrs\u001b[0m (\u001b[33mtme-st\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210aae4a",
   "metadata": {},
   "source": [
    "#### load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "287a68b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type_mapping = {\n",
    "    'Epithelial': ['KRT18'],\n",
    "    'T cell': ['IL7R'],\n",
    "    'Macrophage': ['CD14', 'FCGR3A'],\n",
    "    'DC': ['HLA-DRA', 'ITGAX'],\n",
    "    'Fibroblast': ['BGN'],\n",
    "    'Endothelial': ['PECAM1'],\n",
    "    'Acinar': ['PRSS1'],\n",
    "    'Islet': ['INS']\n",
    "}\n",
    "\n",
    "genes = sorted([v for vs in cell_type_mapping.values() for v in vs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb6be73c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/pdac/HT270P1-S1H1U1',\n",
       " '../data/pdac/HT264P1-S1H2U1',\n",
       " '../data/pdac/HT424P1-H3A1U1',\n",
       " '../data/pdac/HT434P1-S1H3U1',\n",
       " '../data/pdac/HT427P1-S1H1U1',\n",
       " '../data/pdac/HT416P1-S1H1A1U1']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fps = [os.path.join('../data/pdac', x) for x in os.listdir('../data/pdac')]\n",
    "fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "424b0aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = [\n",
    "    'HT270P1-S1H1U1',\n",
    "#     'HT424P1-H3A1U1',\n",
    "# #     'HT434P1-S1H3U1',\n",
    "#     'HT427P1-S1H1U1',\n",
    "]\n",
    "\n",
    "val_samples = [\n",
    "    'HT264P1-S1H2U1',\n",
    "#     'HT416P1-S1H1A1U1'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3761f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scale_expression_counts(a, scale=100.):\n",
    "#     X = a.X.toarray()\n",
    "# #     X = np.log1p(X.toarray())\n",
    "# #      0-1\n",
    "# #     X -= np.expand_dims(X.min(axis=0), 0)\n",
    "# #     X /= np.expand_dims(X.max(axis=0), 0)\n",
    "# #     X *= scale\n",
    "# #     X = X.astype(np.int32).astype(np.float32)\n",
    "#     a.X = X.astype(np.int32)\n",
    "    \n",
    "#     return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0a9ab3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HT270P1-S1H1U1 (3940, 10)\n",
      "HT264P1-S1H2U1 (3234, 10)\n"
     ]
    }
   ],
   "source": [
    "sample_to_adata = {}\n",
    "\n",
    "for fp in fps:\n",
    "    s = fp.split('/')[-1]\n",
    "    if s in train_samples or s in val_samples:\n",
    "        a = sc.read_h5ad(os.path.join(fp, 'adata.h5ad'))\n",
    "        a.X = a.layers['counts'].toarray().astype(np.int32) # using raw counts\n",
    "        a = a[:, genes]\n",
    "#         a = scale_expression_counts(a)\n",
    "        sample_to_adata[s] = a\n",
    "\n",
    "        print(s, a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e6f0cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_to_train_adata = {k:v for k, v in sample_to_adata.items() if k in train_samples}\n",
    "sample_to_val_adata = {k:v for k, v in sample_to_adata.items() if k in val_samples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45bc8cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "View of AnnData object with n_obs × n_vars = 3940 × 10\n",
       "    obs: 'in_tissue', 'array_row', 'array_col', 'clusters', 'spot_index'\n",
       "    var: 'gene_ids', 'feature_types', 'genome', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'\n",
       "    uns: 'full_res_he', 'hvg', 'leiden', 'log1p', 'neighbors', 'nuclei_segmentation_1X_notrim', 'nuclei_segmentation_1X_trimmed', 'pca', 'rescaled_he', 'rescaled_spot_masks', 'rescaled_spot_metadata', 'segmented_nuclei_coords', 'segmented_nuclei_coords_1X_notrim', 'segmented_nuclei_coords_1X_trimmed', 'spatial', 'trimmed', 'umap'\n",
       "    obsm: 'X_pca', 'X_umap', 'spatial', 'spatial_16X_notrim', 'spatial_16X_trimmed', 'spatial_1X_notrim', 'spatial_1X_trimmed', 'spatial_2X_notrim', 'spatial_2X_trimmed', 'spatial_4X_notrim', 'spatial_4X_trimmed', 'spatial_8X_notrim', 'spatial_8X_trimmed', 'spatial_trimmed'\n",
       "    varm: 'PCs'\n",
       "    layers: 'counts'\n",
       "    obsp: 'connectivities', 'distances'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = next(iter(sample_to_train_adata.values()))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9daaa05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in_tissue</th>\n",
       "      <th>array_row</th>\n",
       "      <th>array_col</th>\n",
       "      <th>clusters</th>\n",
       "      <th>spot_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAACAACGAATAGTTC-1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACAAGTATCTCCCA-1</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>102</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACAATCTACTAGCA-1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACACCAATAACTGC-1</th>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACAGAGCGACTCCT-1</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>94</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTGTTGTGTGTCAAGA-1</th>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>77</td>\n",
       "      <td>17</td>\n",
       "      <td>3936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTGTTTCACATCCAGG-1</th>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "      <td>3937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTGTTTCATTAGTCTA-1</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>3938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTGTTTCCATACAACT-1</th>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>3939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTGTTTGTGTAAATTC-1</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>3940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3940 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    in_tissue  array_row  array_col clusters  spot_index\n",
       "AAACAACGAATAGTTC-1          1          0         16       17           1\n",
       "AAACAAGTATCTCCCA-1          1         50        102        3           2\n",
       "AAACAATCTACTAGCA-1          1          3         43        2           3\n",
       "AAACACCAATAACTGC-1          1         59         19        5           4\n",
       "AAACAGAGCGACTCCT-1          1         14         94       14           5\n",
       "...                       ...        ...        ...      ...         ...\n",
       "TTGTTGTGTGTCAAGA-1          1         31         77       17        3936\n",
       "TTGTTTCACATCCAGG-1          1         58         42        5        3937\n",
       "TTGTTTCATTAGTCTA-1          1         60         30        5        3938\n",
       "TTGTTTCCATACAACT-1          1         45         27        1        3939\n",
       "TTGTTTGTGTAAATTC-1          1          7         51        2        3940\n",
       "\n",
       "[3940 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bceeb052",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HETransform(object):\n",
    "    def __init__(self, p=.8, brightness=.1, contrast=.1, saturation=.1, hue=.1, sharpness=.3,\n",
    "                 no_flip=False, no_color=False, normalize=True):\n",
    "        self.brightness = brightness\n",
    "        self.contrast = contrast\n",
    "        self.saturation = saturation\n",
    "        self.hue = hue\n",
    "        self.sharpness = sharpness\n",
    "        self.no_flip = no_flip\n",
    "        self.no_color = no_color\n",
    "        \n",
    "        if normalize:\n",
    "            self.normalize = Normalize((0.771, 0.651, 0.752), (0.229, 0.288, 0.224)) # from HT397B1-H2 ffpe H&E image\n",
    "        else:\n",
    "            self.normalize = nn.Identity()\n",
    " \n",
    "        self.p = p\n",
    "    \n",
    "    def apply_color_transforms(self, x, brightness, contrast, saturation, hue, sharpness):\n",
    "        x = TF.adjust_brightness(x, brightness)\n",
    "        x = TF.adjust_contrast(x, contrast)\n",
    "        x = TF.adjust_saturation(x, saturation)\n",
    "        x = TF.adjust_hue(x, hue)\n",
    "        x = TF.adjust_sharpness(x, sharpness)\n",
    "        return x\n",
    "        \n",
    "    def __call__(self, he, mask):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if isinstance(he, torch.Tensor):\n",
    "            hes = [he]\n",
    "            masks = [mask]\n",
    "            return_type = 'image'\n",
    "        elif isinstance(he, dict):\n",
    "            keys = list(he.keys())\n",
    "            hes = [he[k] for k in keys]\n",
    "            masks = [mask[k] for k in keys]\n",
    "            return_type = 'dict'\n",
    "        else:\n",
    "            hes = he\n",
    "            masks = mask\n",
    "            return_type = 'list'\n",
    "                                \n",
    "        # we apply transforms with probability p\n",
    "        if torch.rand(size=(1,)) < self.p:\n",
    "            if not self.no_color:\n",
    "                brightness, contrast, saturation, hue, sharpness = (\n",
    "                    np.random.uniform(max(0, 1 - self.brightness), 1 + self.brightness, size=1)[0],\n",
    "                    np.random.uniform(max(0, 1 - self.contrast), 1 + self.contrast, size=1)[0],\n",
    "                    np.random.uniform(max(0, 1 - self.saturation), 1 + self.saturation, size=1)[0],\n",
    "                    np.random.uniform(-self.hue, self.hue, size=1)[0],\n",
    "                    np.random.uniform(max(0, 1 - self.sharpness), 1 + self.sharpness, size=1)[0],\n",
    "                )\n",
    "                # apply color jitter and sharpness\n",
    "                hes = [self.apply_color_transforms(x, brightness, contrast, saturation, hue, sharpness)\n",
    "                       for x in hes]\n",
    "            \n",
    "            # vertical and horizontal flips happen with p=.5\n",
    "            do_hflip, do_vflip = torch.rand(size=(2,)) < .5 \n",
    "            if do_hflip and not self.no_flip:\n",
    "                hes = [TF.hflip(x) for x in hes]\n",
    "                masks = [TF.hflip(x) for x in masks]\n",
    "            if do_vflip and not self.no_flip:\n",
    "                hes = [TF.vflip(x) for x in hes]\n",
    "                masks = [TF.vflip(x) for x in masks]\n",
    "        \n",
    "        # normalize he\n",
    "        hes = [self.normalize(x) for x in hes]\n",
    "                    \n",
    "        if return_type == 'image':\n",
    "            return hes[0], masks[0]\n",
    "        elif return_type == 'dict':\n",
    "            return {k:v for k, v in zip(keys, hes)}, {k:v for k, v in zip(keys, masks)}\n",
    "        return hes, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebae8d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflection_mosiac(x, border=256, dtype=torch.int32):\n",
    "    max_r, max_c = x.shape[-2], x.shape[-1]\n",
    "    if len(x.shape) == 3:\n",
    "        mosaic = torch.zeros((x.shape[0], max_r + (border * 2), max_c + (border * 2))).to(dtype)\n",
    "    else:\n",
    "        mosaic = torch.zeros((max_r + (border * 2), max_c + (border * 2))).to(dtype)\n",
    "    \n",
    "    # make tiles\n",
    "    top_left = TF.pad(x, padding=[border, border, 0, 0], padding_mode='reflect')\n",
    "    top_right = TF.pad(x, padding=[0, border, border, 0], padding_mode='reflect')\n",
    "    bottom_left = TF.pad(x, padding=[border, 0, 0, border], padding_mode='reflect')\n",
    "    bottom_right = TF.pad(x, padding=[0, 0, border, border], padding_mode='reflect')\n",
    "    \n",
    "    if len(x.shape) == 3:\n",
    "        mosaic[:, :max_r + border, :max_c + border] = top_left\n",
    "        mosaic[:, :max_r + border, border:] = top_right\n",
    "        mosaic[:, border:, :max_c + border] = bottom_left\n",
    "        mosaic[:, border:, border:] = bottom_right\n",
    "    else:\n",
    "        mosaic[:max_r + border, :max_c + border] = top_left\n",
    "        mosaic[:max_r + border, border:] = top_right\n",
    "        mosaic[border:, :max_c + border] = bottom_left\n",
    "        mosaic[border:, border:] = bottom_right\n",
    "    \n",
    "    return mosaic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "737a80e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_dicts(adata, keys=('2X', '8X'), border=256, mode='spot'):\n",
    "    he_dict = {}\n",
    "    for k, x in adata.uns['rescaled_he'].items():\n",
    "        if re.findall(r'^[0-9]+X.*$', k) and 'trimmed' in k:\n",
    "            scale = int(re.sub(r'^([0-9]+)X.*$', r'\\1', k))\n",
    "            x = torch.tensor(rearrange(x, 'h w c -> c h w')).to(torch.uint8)\n",
    "            x = reflection_mosiac(x, border=border, dtype=torch.uint8)\n",
    "            x = rearrange(x, 'c h w -> h w c').numpy().astype(np.uint8)\n",
    "            he_dict[f'{scale}X'] = x\n",
    "            \n",
    "    mask_dict = {}\n",
    "    for k, x in adata.uns['rescaled_spot_masks'].items():\n",
    "        if re.findall(r'^[0-9]+X.*$', k) and 'trimmed' in k:\n",
    "            scale = int(re.sub(r'^([0-9]+)X.*$', r'\\1', k))\n",
    "            x = torch.tensor(x.astype(np.int32))\n",
    "            x = reflection_mosiac(x, border=border, dtype=torch.int32)\n",
    "            x = x.numpy().astype(np.int32)\n",
    "            mask_dict[f'{scale}X'] = x\n",
    "            \n",
    "    if mode == 'hex':\n",
    "        mask_dict = {k:convert_spot_masks(adata, mask=v, key=f'{k}_trimmed', mode='hex')\n",
    "                     for k, v in mask_dict.items()}\n",
    "            \n",
    "    he_dict = {k:v for k, v in he_dict.items() if k in keys}\n",
    "    mask_dict = {k:v for k, v in mask_dict.items() if k in keys}\n",
    "    \n",
    "    return he_dict, mask_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "96a73db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rgb(img):\n",
    "    if img.shape[-1] == 3:\n",
    "        img = rearrange(img, 'h w c -> c h w')\n",
    "        \n",
    "    if not isinstance(img, torch.Tensor):\n",
    "        img = torch.tensor(img)\n",
    "        \n",
    "    if img.dtype == torch.uint8:\n",
    "        img = TF.convert_image_dtype(img, dtype=torch.float32)\n",
    "    \n",
    "    if img.max() > 1.:\n",
    "        img -= img.min()\n",
    "        img /= img.max()\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "def create_color_augmentations(he, he_context, transform, n=10, dtype=torch.uint8):\n",
    "    aug = torch.zeros(n, he.shape[0], he.shape[1], he.shape[2], dtype=dtype)\n",
    "    aug_context = torch.zeros(n, he_context.shape[0], he_context.shape[1], he_context.shape[2], dtype=dtype)\n",
    "    # one at a time to keep RAM down\n",
    "    for i in range(n):\n",
    "        (aug_he, aug_he_context), _ = transform([he, he_context], None)\n",
    "        \n",
    "        # rescale to 0-1\n",
    "        aug_he -= aug_he.min()\n",
    "        aug_he /= aug_he.max()\n",
    "        aug_he_context -= aug_he_context.min()\n",
    "        aug_he_context /= aug_he_context.max()\n",
    "        \n",
    "        # back to dtype\n",
    "        aug_he = TF.convert_image_dtype(aug_he, dtype)\n",
    "        aug_he_context = TF.convert_image_dtype(aug_he_context, dtype)\n",
    "        \n",
    "        aug[i] = aug_he\n",
    "        aug_context[i] = aug_he_context\n",
    "    \n",
    "    return aug, aug_context\n",
    "\n",
    "\n",
    "def create_masks(labeled_mask, max_area, thresh=.5):\n",
    "    voxel_idxs = torch.unique(labeled_mask)[1:].numpy().astype(int)\n",
    "    masks = torch.zeros((len(voxel_idxs), labeled_mask.shape[0], labeled_mask.shape[1]), dtype=torch.bool)\n",
    "    for i, l in enumerate(voxel_idxs):\n",
    "        m = masks[i]\n",
    "        m[labeled_mask==l] = 1\n",
    "\n",
    "    keep = masks.sum(dim=(-1,-2)) / max_area > thresh\n",
    "    masks = masks[keep]\n",
    "    voxel_idxs = voxel_idxs[keep]\n",
    "        \n",
    "    return masks, voxel_idxs\n",
    "        \n",
    "\n",
    "class STDataset(Dataset):\n",
    "    \"\"\"ST Dataset\"\"\"\n",
    "    def __init__(self, adata, he, he_context, labeled_mask, context_scaler, coordinate_key,\n",
    "                 tile_size=512, he_color_transform=None, he_post_color_transform=None,\n",
    "                 border=512, max_jitter=0., n_augmentations=10, min_voxel_fraction=.5, normalize=True):\n",
    "        self.spot_ids, _ = zip(*sorted([(sid, sidx) for sid, sidx in zip(adata.obs.index, adata.obs['spot_index'])],\n",
    "                                    key=lambda x: x[1]))\n",
    "        self.spot_ids = np.asarray(self.spot_ids)\n",
    "        self.adata = adata[self.spot_ids]\n",
    "        self.context_scaler = context_scaler\n",
    "        self.he = convert_rgb(he)\n",
    "        self.he_context = convert_rgb(he_context)\n",
    "        self.labeled_mask = torch.tensor(labeled_mask, dtype=torch.int32) if not isinstance(labeled_mask, torch.Tensor) else labeled_mask\n",
    "        \n",
    "        self.he_color_transform = he_color_transform\n",
    "        self.he_post_color_transform = he_post_color_transform\n",
    "        \n",
    "        self.n_augmentations = n_augmentations\n",
    "        if self.n_augmentations is not None:\n",
    "            self.aug_he, self.aug_he_context = create_color_augmentations(\n",
    "                self.he, self.he_context, self.he_color_transform,\n",
    "                n=self.n_augmentations, dtype=torch.uint8)\n",
    "        else:\n",
    "            self.aug_he = rearrange(self.he, 'c h w -> 1 c h w')\n",
    "            self.aug_he_context = rearrange(self.he_context, 'c h w -> 1 c h w')\n",
    "            \n",
    "        if normalize:\n",
    "            self.normalize = Normalize((0.771, 0.651, 0.752), (0.229, 0.288, 0.224)) # from HT397B1-H2 ffpe H&E image\n",
    "        else:\n",
    "            self.normalize = nn.Identity()\n",
    "                \n",
    "        self.exp = torch.tensor(adata.X, dtype=torch.int32)\n",
    "\n",
    "        self.tile_size = tile_size \n",
    "        \n",
    "        self.offset = int(self.tile_size // 2 + 1)\n",
    "        self.border = border\n",
    "        self.max_jitter = max_jitter\n",
    "  \n",
    "        self.pixel_coords = np.asarray([[int(r), int(c)] for c, r in self.adata.obsm[coordinate_key]])\n",
    "    \n",
    "        idxs = np.random.choice(np.arange(self.pixel_coords.shape[0]), size=1000)\n",
    "        max_spots = 0\n",
    "        for i in idxs:\n",
    "            r, c = self.pixel_coords[i]\n",
    "            r, c = r + self.border, c + self.border # adjust for reflection padding\n",
    "            r, c = r - self.offset, c - self.offset # adjust from center to top left\n",
    "            lm = TF.crop(self.labeled_mask, top=r, left=c, height=self.tile_size, width=self.tile_size)\n",
    "            m, _ = create_masks(lm, 1000, thresh=.0001)\n",
    "            m = m[m.sum(dim=(-1, -2))>0]\n",
    "            max_spots = max(max_spots, m.shape[0])\n",
    "    \n",
    "        self.max_spots = 2**int(np.log2(max_spots) + 1)\n",
    "        \n",
    "        r, c = self.he.shape[1] // 2, self.he.shape[2] // 2\n",
    "        tile = TF.crop(self.labeled_mask, top=r, left=c, height=self.tile_size, width=self.tile_size)\n",
    "        self.max_voxel_area = np.max(\n",
    "            regionprops_table(tile.detach().numpy(), properties=['label', 'area'])['area'])\n",
    "        self.min_voxel_fraction = min_voxel_fraction\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pixel_coords)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        r, c = self.pixel_coords[idx]\n",
    "        r = int(r + np.random.uniform(-self.max_jitter, self.max_jitter))\n",
    "        c = int(c + np.random.uniform(-self.max_jitter, self.max_jitter))\n",
    "        r_context, c_context = r // self.context_scaler, c // self.context_scaler\n",
    "        \n",
    "        # add offset and border\n",
    "        r, c = r + self.border - self.offset, c + self.border - self.offset\n",
    "        r_context, c_context = r_context + self.border - self.offset, c_context + self.border - self.offset\n",
    "        \n",
    "        if self.n_augmentations is not None:\n",
    "            i = torch.randint(0, self.n_augmentations, (1,)).item()\n",
    "        else:\n",
    "            i = 0\n",
    "            \n",
    "        he = TF.crop(self.aug_he[i], top=r, left=c, height=self.tile_size, width=self.tile_size)\n",
    "        he_context = TF.crop(self.aug_he_context[i], top=r_context, left=c_context,\n",
    "                             height=self.tile_size, width=self.tile_size)\n",
    "        \n",
    "        # we need to go to float32\n",
    "        he = TF.convert_image_dtype(he, torch.float32)\n",
    "        he_context = TF.convert_image_dtype(he_context, torch.float32)\n",
    "        \n",
    "        # create mask\n",
    "        lm = TF.crop(self.labeled_mask, top=r, left=c, height=self.tile_size, width=self.tile_size)\n",
    "        m, v_idxs = create_masks(lm, self.max_voxel_area, thresh=self.min_voxel_fraction)\n",
    "        idxs = v_idxs - 1\n",
    "        masks = torch.zeros((self.max_spots, m.shape[1], m.shape[2]), dtype=m.dtype)\n",
    "        masks[:len(idxs)] = m\n",
    "        \n",
    "        # post color augs\n",
    "        if self.he_post_color_transform is not None:\n",
    "            (he, he_context), (masks,) = self.he_post_color_transform([he, he_context], [masks])\n",
    "        \n",
    "        # exp\n",
    "        exp = torch.zeros((self.max_spots, self.exp.shape[1]), dtype=self.exp.dtype)\n",
    "        exp[:len(idxs)] = self.exp[idxs]\n",
    "        \n",
    "        voxel_idxs = torch.zeros((self.max_spots,), dtype=torch.int32)\n",
    "        voxel_idxs[:len(idxs)] = torch.tensor(v_idxs, dtype=torch.int32)\n",
    "        \n",
    "        return {\n",
    "            'he': self.normalize(he),\n",
    "            'he_context': self.normalize(he_context),\n",
    "            'he_orig': he,\n",
    "            'masks': masks,\n",
    "            'voxel_idxs': voxel_idxs,\n",
    "            'exp': exp,\n",
    "            'n_voxels': len(idxs)\n",
    "        }\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a934162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = [2, 8]\n",
    "keys = [f'{s}X' for s in scales]\n",
    "mode = 'spot'\n",
    "tile_size = 256\n",
    "spot_radius = next(iter(sample_to_adata.values())).uns['rescaled_spot_metadata'][keys[0] + '_trimmed']['spot_radius']\n",
    "jitter = int(spot_radius * 2)\n",
    "border = jitter * 10\n",
    "\n",
    "train_he_color_transform = HETransform(p=.95, brightness=.1, contrast=.1, saturation=.1, hue=.1,\n",
    "                                 no_flip=True, no_color=False, normalize=False)\n",
    "train_he_post_color_transform = HETransform(p=.95, no_flip=False, no_color=True, normalize=False)\n",
    "\n",
    "sample_to_train_ds = {}\n",
    "for s in train_samples:\n",
    "    a = sample_to_adata[s]\n",
    "\n",
    "    he_dict, mask_dict = get_img_dicts(a, keys=keys, border=border, mode=mode)\n",
    "\n",
    "    ds = STDataset(\n",
    "        a, he_dict[keys[0]], he_dict[keys[1]], mask_dict[keys[0]], scales[1] // scales[0], 'spatial_2X_trimmed',\n",
    "        tile_size=tile_size, he_color_transform=train_he_color_transform, \n",
    "        he_post_color_transform=train_he_post_color_transform,\n",
    "        max_jitter=jitter, border=border, n_augmentations=2, min_voxel_fraction=.5, normalize=True)\n",
    "    \n",
    "    sample_to_train_ds[s] = ds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e60278",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_he_color_transform = HETransform(p=0., normalize=False)\n",
    "val_he_post_color_transform = HETransform(p=0., normalize=False)\n",
    "\n",
    "sample_to_val_ds = {}\n",
    "for s in val_samples:\n",
    "    a = sample_to_adata[s]\n",
    "\n",
    "    he_dict, mask_dict = get_img_dicts(a, keys=keys, border=border, mode=mode)\n",
    "\n",
    "    ds = STDataset(\n",
    "        a, he_dict[keys[0]], he_dict[keys[1]], mask_dict[keys[0]], scales[1] // scales[0], 'spatial_2X_trimmed',\n",
    "        tile_size=tile_size, he_color_transform=val_he_color_transform, \n",
    "        he_post_color_transform=val_he_post_color_transform,\n",
    "        max_jitter=0., border=border, n_augmentations=None, min_voxel_fraction=.5, normalize=True)\n",
    "    \n",
    "    sample_to_val_ds[s] = ds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b09a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultisampleSTDataset(Dataset):\n",
    "    def __init__(self, ds_dict):\n",
    "        super().__init__()\n",
    "        self.samples = list(ds_dict.keys())\n",
    "        self.ds_dict = ds_dict\n",
    "        \n",
    "        self.mapping = [(k, i) for k, ds in ds_dict.items() for i in range(len(ds))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.mapping)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        k, i = self.mapping[idx]\n",
    "        \n",
    "        d = self.ds_dict[k][i]\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d063d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MultisampleSTDataset(sample_to_train_ds)\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d12c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = MultisampleSTDataset(sample_to_val_ds)\n",
    "len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619da1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=1)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3daacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f4e507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105f2694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2cf46a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = next(iter(sample_to_train_ds.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3a6d73a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "db6bef4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 256])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['he'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695ab027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eb801e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "61ecf5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rgb(img):\n",
    "    if img.shape[-1] == 3:\n",
    "        img = rearrange(img, 'h w c -> c h w')\n",
    "        \n",
    "    if not isinstance(img, torch.Tensor):\n",
    "        img = torch.tensor(img)\n",
    "        \n",
    "    if torch.max(img)>1.1:\n",
    "        img = TF.convert_image_dtype(img, dtype=torch.float32)\n",
    "            \n",
    "    return img\n",
    "\n",
    "\n",
    "def generate_voxel_masks(labeled_mask, padded_spot_idxs, n_voxels):\n",
    "    masks = torch.zeros((padded_spot_idxs.shape[0], labeled_mask.shape[0], labeled_mask.shape[1]))\n",
    "    for i, l in enumerate(padded_spot_idxs[:n_voxels]):\n",
    "        m = labeled_mask.clone().detach()\n",
    "        m[m!=l] = 0.\n",
    "        m[m==l] = 1.\n",
    "        masks[i] = m\n",
    "    return masks\n",
    "\n",
    "\n",
    "def generate_padded_exp(adata, spot_idxs, max_spots, masks, pixels_per_voxel, use_raw=False):\n",
    "    if use_raw:\n",
    "        x = adata.raw.X[spot_idxs]\n",
    "    else:\n",
    "        x = adata.X[spot_idxs]\n",
    "\n",
    "    if 'sparse' in str(type(x)).lower():\n",
    "        x = x.toarray()\n",
    "\n",
    "    padded_exp = torch.zeros((max_spots, x.shape[1]))\n",
    "    padded_exp[:x.shape[0]] = torch.tensor(x)\n",
    "\n",
    "    padded_exp *= (masks.sum(dim=(1,2)).unsqueeze(dim=-1) + 1.) / pixels_per_voxel\n",
    "    padded_exp = torch.round(padded_exp)\n",
    "    \n",
    "    return padded_exp\n",
    "\n",
    "def generate_expression_tiles(masks, exp, n_voxels, tile_res=32):    \n",
    "    tiles = torch.zeros(masks.shape[-2],\n",
    "                        masks.shape[-1],\n",
    "                        exp.shape[-1])\n",
    "    \n",
    "    for m, e in zip(masks[:n_voxels], exp[:n_voxels]):\n",
    "        tiles[m==1] = e \n",
    "        \n",
    "    tiles = reduce(tiles, '(h1 h2) (w1 w2) c -> h1 w1 c', 'mean', h2=tile_res, w2=tile_res)\n",
    "        \n",
    "    return tiles\n",
    "\n",
    "def get_n_voxels(padded_voxel_idxs):\n",
    "    if padded_voxel_idxs.sum() == 0:\n",
    "        return 0\n",
    "    idx = padded_voxel_idxs.flip((0,)).nonzero()[0].item()\n",
    "    return len(padded_voxel_idxs[:-idx])\n",
    "\n",
    "\n",
    "class STDataset(Dataset):\n",
    "    \"\"\"ST Dataset\"\"\"\n",
    "    def __init__(self, adata, he_dict, mask_dict, coordinate_key,\n",
    "                 tile_sizes=512, use_raw=False, he_transform=None,\n",
    "                 border=512, max_jitter=0.,\n",
    "                 tile_res=32, normalize=True):\n",
    "        \"\"\"\n",
    "        adata: AnnData object\n",
    "            - .X must be unnormalized counts\n",
    "            - must have column in .obs['spot_index'] that specified the spot index in the scaled mask dict\n",
    "        scaled_he_dict: dict\n",
    "            - values are rgb H&E images, keys are '[0-9]+X', where the integer in front of X is the scale factor of the H&E image.\n",
    "        scaled_mask_dict: dict\n",
    "            - values are labeled images where 0 is background and all other pixels coorespond to index stored in .obs['spot_index'].\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # make sure we are ordered by spot index\n",
    "        self.spot_ids, _ = zip(*sorted([(sid, sidx) for sid, sidx in zip(adata.obs.index, adata.obs['spot_index'])],\n",
    "                                    key=lambda x: x[1]))\n",
    "        self.spot_ids = np.asarray(self.spot_ids)\n",
    "        self.adata = adata[self.spot_ids]\n",
    "        self.he_dict = {k:convert_rgb(v) for k, v in he_dict.items()}\n",
    "        self.mask_dict = {k:torch.tensor(v).to(torch.int32) if not isinstance(v, torch.Tensor) else v\n",
    "                                 for k, v in mask_dict.items()}\n",
    "        self.scales = sorted([int(re.sub(r'^([0-9]+)X$', r'\\1', k)) for k in self.he_dict.keys()])\n",
    "        self.tile_res = tile_res\n",
    "        \n",
    "        if normalize:\n",
    "            self.normalize = Normalize((0.771, 0.651, 0.752), (0.229, 0.288, 0.224)) # from HT397B1-H2 ffpe H&E image\n",
    "        else:\n",
    "            self.normalize = nn.Identity()\n",
    "        \n",
    "        if isinstance(tile_sizes, int):\n",
    "            self.tile_sizes = [tile_sizes] * len(self.scales)\n",
    "        else:\n",
    "            self.tile_sizes = tile_sizes # defines the size of crops to be taken from each h&e resolution\n",
    "        \n",
    "        _, n_row, n_col = self.he_dict[str(self.scales[0]) + 'X'].shape\n",
    "        self.offset = int(self.tile_sizes[0] // 2 + 1)\n",
    "        self.border = border\n",
    "        self.max_jitter = max_jitter\n",
    "  \n",
    "        self.pixel_coords = np.asarray([[int(c), int(r)] for r, c in self.adata.obsm[coordinate_key]])\n",
    "\n",
    "        self.he_transform = he_transform\n",
    "\n",
    "        # expression related\n",
    "        self.use_raw = use_raw\n",
    "        \n",
    "        idxs = np.random.choice(np.arange(self.pixel_coords.shape[0]), size=1000)\n",
    "        key = str(self.scales[0]) + 'X'\n",
    "        max_spots = 0\n",
    "        self.pixels_per_voxel = 0\n",
    "        for i in idxs:\n",
    "            mask = self.mask_dict[key]\n",
    "            r, c = self.pixel_coords[i]\n",
    "            r, c = r + self.border, c + self.border # adjust for reflection padding\n",
    "            r, c = r - self.offset, c - self.offset # adjust from center to top left\n",
    "            m = TF.crop(mask, top=r, left=c, height=self.tile_sizes[0], width=self.tile_sizes[0])\n",
    "            max_spots = max(max_spots, len(np.unique(m)))\n",
    "        self.max_spots = 2**int(np.log2(max_spots) + 1)\n",
    "        r, c = n_row // 2, n_col // 2\n",
    "        tile = self.mask_dict[key][r:r + self.tile_sizes[0] * 2, c:c + self.tile_sizes[0] * 2]\n",
    "        self.pixels_per_voxel = np.max(\n",
    "            regionprops_table(tile.detach().numpy(), properties=['label', 'area'])['area'])\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pixel_coords)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        r, c = self.pixel_coords[idx]\n",
    "        r = int(r + np.random.uniform(-self.max_jitter, self.max_jitter))\n",
    "        c = int(c + np.random.uniform(-self.max_jitter, self.max_jitter))\n",
    "        initial = self.scales[0]\n",
    "        scale_to_coords = {s: (\n",
    "                               int((r / (s / initial)) - self.offset + self.border),\n",
    "                               int((c / (s / initial)) - self.offset + self.border)\n",
    "                           )\n",
    "                           for i, s in enumerate(self.scales)}\n",
    "        he_tile_dict, mask_tile_dict = {}, {}\n",
    "        for scale, tile_size in zip(self.scales, self.tile_sizes):\n",
    "            key = f'{scale}X'\n",
    "            he, mask = self.he_dict[key], self.mask_dict[key]\n",
    "            r, c = scale_to_coords[scale]\n",
    "            he_tile_dict[key] = TF.crop(he, top=r, left=c, height=tile_size, width=tile_size)\n",
    "            mask_tile_dict[key] = TF.crop(mask, top=r, left=c, height=tile_size, width=tile_size)\n",
    "            \n",
    "        if self.he_transform is not None:\n",
    "            he_tile_dict, mask_tile_dict = self.he_transform(he_tile_dict, mask_tile_dict)\n",
    "        spot_idxs = torch.unique(mask_tile_dict[str(self.scales[0]) + 'X']).numpy() - 1\n",
    "        if spot_idxs[0] == -1:\n",
    "            spot_idxs = spot_idxs[1:] # drop first value, which is background\n",
    "        padded_spot_idxs = np.asarray([0] * self.max_spots)\n",
    "        padded_spot_idxs[:spot_idxs.shape[0]] = spot_idxs + 1\n",
    "        masks = generate_voxel_masks(mask_tile_dict[str(self.scales[0]) + 'X'],\n",
    "                                     padded_spot_idxs, len(spot_idxs))\n",
    "        padded_exp = generate_padded_exp(self.adata, spot_idxs, self.max_spots,\n",
    "                                         masks, self.pixels_per_voxel, use_raw=self.use_raw)\n",
    "        \n",
    "        padded_spot_idxs = torch.tensor(padded_spot_idxs, dtype=torch.int16)\n",
    "        \n",
    "        return {\n",
    "            'he': self.normalize(he_tile_dict['2X']),\n",
    "            'he_context': self.normalize(he_tile_dict['8X']),\n",
    "            'he_orig': he_tile_dict['2X'],\n",
    "            'masks': masks.to(torch.bool),\n",
    "            'voxel_idxs': padded_spot_idxs,\n",
    "            'exp': padded_exp.to(torch.int32),\n",
    "            'n_voxels': get_n_voxels(padded_spot_idxs)\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "19a2854e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflection_mosiac(x, border=256, dtype=torch.int32):\n",
    "    max_r, max_c = x.shape[-2], x.shape[-1]\n",
    "    if len(x.shape) == 3:\n",
    "        mosaic = torch.zeros((x.shape[0], max_r + (border * 2), max_c + (border * 2))).to(dtype)\n",
    "    else:\n",
    "        mosaic = torch.zeros((max_r + (border * 2), max_c + (border * 2))).to(dtype)\n",
    "    \n",
    "    # make tiles\n",
    "    top_left = TF.pad(x, padding=[border, border, 0, 0], padding_mode='reflect')\n",
    "    top_right = TF.pad(x, padding=[0, border, border, 0], padding_mode='reflect')\n",
    "    bottom_left = TF.pad(x, padding=[border, 0, 0, border], padding_mode='reflect')\n",
    "    bottom_right = TF.pad(x, padding=[0, 0, border, border], padding_mode='reflect')\n",
    "    \n",
    "    if len(x.shape) == 3:\n",
    "        mosaic[:, :max_r + border, :max_c + border] = top_left\n",
    "        mosaic[:, :max_r + border, border:] = top_right\n",
    "        mosaic[:, border:, :max_c + border] = bottom_left\n",
    "        mosaic[:, border:, border:] = bottom_right\n",
    "    else:\n",
    "        mosaic[:max_r + border, :max_c + border] = top_left\n",
    "        mosaic[:max_r + border, border:] = top_right\n",
    "        mosaic[border:, :max_c + border] = bottom_left\n",
    "        mosaic[border:, border:] = bottom_right\n",
    "    \n",
    "    return mosaic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cd08b633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_dicts(adata, keys=('2X', '8X'), border=256, mode='spot'):\n",
    "    he_dict = {}\n",
    "    for k, x in adata.uns['rescaled_he'].items():\n",
    "        if re.findall(r'^[0-9]+X.*$', k) and 'trimmed' in k:\n",
    "            scale = int(re.sub(r'^([0-9]+)X.*$', r'\\1', k))\n",
    "            x = torch.tensor(rearrange(x, 'h w c -> c h w')).to(torch.uint8)\n",
    "            x = reflection_mosiac(x, border=border, dtype=torch.uint8)\n",
    "            x = rearrange(x, 'c h w -> h w c').numpy().astype(np.uint8)\n",
    "            he_dict[f'{scale}X'] = x\n",
    "            \n",
    "    mask_dict = {}\n",
    "    for k, x in adata.uns['rescaled_spot_masks'].items():\n",
    "        if re.findall(r'^[0-9]+X.*$', k) and 'trimmed' in k:\n",
    "            scale = int(re.sub(r'^([0-9]+)X.*$', r'\\1', k))\n",
    "            x = torch.tensor(x.astype(np.int32))\n",
    "            x = reflection_mosiac(x, border=border, dtype=torch.int32)\n",
    "            x = x.numpy().astype(np.int32)\n",
    "            mask_dict[f'{scale}X'] = x\n",
    "            \n",
    "    if mode == 'hex':\n",
    "        mask_dict = {k:convert_spot_masks(adata, mask=v, key=f'{k}_trimmed', mode='hex')\n",
    "                     for k, v in mask_dict.items()}\n",
    "            \n",
    "    he_dict = {k:v for k, v in he_dict.items() if k in keys}\n",
    "    mask_dict = {k:v for k, v in mask_dict.items() if k in keys}\n",
    "    \n",
    "    return he_dict, mask_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3ce92210",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['2X', '8X']\n",
    "mode = 'spot'\n",
    "tile_size = 256\n",
    "spot_radius = next(iter(sample_to_adata.values())).uns['rescaled_spot_metadata'][keys[0] + '_trimmed']['spot_radius']\n",
    "jitter = int(spot_radius * 2)\n",
    "border = jitter * 10\n",
    "# border = spot_radius * 4\n",
    "\n",
    "train_he_transform = HETransform(p=.95, brightness=.1, contrast=.1, saturation=.1, hue=.1, normalize=False)\n",
    "\n",
    "sample_to_train_ds = {}\n",
    "for s in train_samples:\n",
    "    a = sample_to_adata[s]\n",
    "\n",
    "    he_dict, mask_dict = get_img_dicts(a, keys=keys, border=border, mode=mode)\n",
    "\n",
    "    ds = STDataset(a, he_dict, mask_dict, 'spatial_2X_trimmed',\n",
    "                     tile_sizes=tile_size, he_transform=train_he_transform,\n",
    "                     max_jitter=jitter, border=border, normalize=True)\n",
    "    \n",
    "    sample_to_train_ds[s] = ds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f5f130c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_he_transform = HETransform(p=0.0, normalize=False)\n",
    "\n",
    "sample_to_val_ds = {}\n",
    "for s in val_samples:\n",
    "    a = sample_to_adata[s]\n",
    "    he_dict, mask_dict = get_img_dicts(a, keys=keys, border=border, mode=mode)\n",
    "    \n",
    "    ds = STDataset(a, he_dict, mask_dict, 'spatial_2X_trimmed',\n",
    "                   tile_sizes=tile_size, he_transform=val_he_transform,\n",
    "                   max_jitter=0., border=border, normalize=True)\n",
    "    \n",
    "    sample_to_val_ds[s] = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "19a943b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultisampleSTDataset(Dataset):\n",
    "    def __init__(self, ds_dict):\n",
    "        super().__init__()\n",
    "        self.samples = list(ds_dict.keys())\n",
    "        self.ds_dict = ds_dict\n",
    "        \n",
    "        self.mapping = [(k, i) for k, ds in ds_dict.items() for i in range(len(ds))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.mapping)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        k, i = self.mapping[idx]\n",
    "        \n",
    "        d = self.ds_dict[k][i]\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "62352413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3940"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = MultisampleSTDataset(sample_to_train_ds)\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "991aed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# time.sleep(60 * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d053d659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3234"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds = MultisampleSTDataset(sample_to_val_ds)\n",
    "len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0c827ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=1)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "49b7bc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.3 ms, sys: 112 ms, total: 153 ms\n",
      "Wall time: 2.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad2271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sample_to_adata[train_samples[0]]\n",
    "a.var.index.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cec3f30",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787dfb6f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cte(padded_exp, masks, n_voxels):\n",
    "    tile = torch.zeros((masks.shape[1], masks.shape[2], padded_exp.shape[1]))\n",
    "    for exp, m in list(zip(padded_exp, masks))[:n_voxels]:\n",
    "        tile[m==1] = exp.to(torch.float32)\n",
    "    return tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbdf8ce",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    print(i)\n",
    "    img = rearrange(train_ds[i]['he_orig'], 'c h w -> h w c')\n",
    "    img -= img.min()\n",
    "    img /= img.max()\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8646c1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# i = 166\n",
    "# i = 66\n",
    "# i = 22\n",
    "i = 14\n",
    "d = train_ds[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb596e12",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img = rearrange(d['he'], 'c h w -> h w c')\n",
    "img -= img.min()\n",
    "img /= img.max()\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc79b4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img = rearrange(d['he_orig'], 'c h w -> h w c')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7271895c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(torch.sum(d['masks'], dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20504c5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for j in range(d['n_voxels']):\n",
    "    plt.imshow(d['masks'][j])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ff8a4e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d['masks'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6d20d0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.sum(d['masks'], dim=(-1, -2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a39a4e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a.var.index.to_list().index('KRT18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3798644",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gene = 'IL7R'\n",
    "recon = cte(d['exp'], d['masks'], d['n_voxels'])\n",
    "plt.imshow(recon[:, :, a.var.index.to_list().index(gene)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999d0243",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gene = 'KRT18'\n",
    "recon = cte(d['exp'], d['masks'], d['n_voxels'])\n",
    "plt.imshow(recon[:, :, a.var.index.to_list().index(gene)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a55d625",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gene = 'PECAM1'\n",
    "recon = cte(d['exp'], d['masks'], d['n_voxels'])\n",
    "plt.imshow(recon[:, :, a.var.index.to_list().index(gene)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ead9ec8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d['exp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a56200",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d['voxel_idxs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc378578",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pool = set(d['voxel_idxs'][:d['n_voxels']].detach().numpy())\n",
    "a.obs['highlight'] = ['yes' if i in pool else 'no'\n",
    "                               for i in a.obs['spot_index']]\n",
    "sc.pl.spatial(a, color='highlight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b397e3e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sc.pl.spatial(a, color='IL7R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c7c0c4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56339c40",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img = rearrange(val_ds[i]['he_orig'], 'c h w -> h w c')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b40db0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gene = 'EPCAM'\n",
    "recon = cte(val_ds[i]['exp'], val_ds[i]['masks'], val_ds[i]['n_voxels'])\n",
    "plt.imshow(recon[:, :, val_adata.var.index.to_list().index(gene)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa221d2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pool = set(val_ds[i]['voxel_idxs'][:val_ds[i]['n_voxels']].detach().numpy())\n",
    "val_adata.obs['highlight'] = ['yes' if i in pool else 'no'\n",
    "                               for i in val_adata.obs['spot_index']]\n",
    "sc.pl.spatial(val_adata, color='highlight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cce34b5",
   "metadata": {},
   "source": [
    "#### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621d11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "modified from https://gist.github.com/rwightman/f8b24f4e6f5504aba03e999e02460d31\n",
    "\"\"\"\n",
    "class Unet(nn.Module):\n",
    "    \"\"\"Unet is a fully convolution neural network for image semantic segmentation\n",
    "    Args:\n",
    "        encoder_name: name of classification model (without last dense layers) used as feature\n",
    "            extractor to build segmentation model.\n",
    "        encoder_weights: one of ``None`` (random initialization), ``imagenet`` (pre-training on ImageNet).\n",
    "        decoder_channels: list of numbers of ``Conv2D`` layer filters in decoder blocks\n",
    "        decoder_use_batchnorm: if ``True``, ``BatchNormalisation`` layer between ``Conv2D`` and ``Activation`` layers\n",
    "            is used.\n",
    "        num_classes: a number of classes for output (output shape - ``(batch, classes, h, w)``).\n",
    "        center: if ``True`` add ``Conv2dReLU`` block on encoder head\n",
    "    NOTE: This is based off an old version of Unet in https://github.com/qubvel/segmentation_models.pytorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            backbone='resnet34',\n",
    "            backbone_kwargs=None,\n",
    "            backbone_indices=None,\n",
    "            decoder_use_batchnorm=True,\n",
    "            decoder_channels=(256, 128, 64, 32, 16),\n",
    "            in_chans=1,\n",
    "            num_classes=5,\n",
    "            center=False,\n",
    "            norm_layer=nn.BatchNorm2d,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        backbone_kwargs = backbone_kwargs or {}\n",
    "        # NOTE some models need different backbone indices specified based on the alignment of features\n",
    "        # and some models won't have a full enough range of feature strides to work properly.\n",
    "        encoder = create_model(\n",
    "            backbone, features_only=True, out_indices=backbone_indices, in_chans=in_chans,\n",
    "            pretrained=False, **backbone_kwargs)\n",
    "        encoder_channels = encoder.feature_info.channels()[::-1]\n",
    "        self.encoder = encoder\n",
    "\n",
    "        if not decoder_use_batchnorm:\n",
    "            norm_layer = None\n",
    "        self.decoder = UnetDecoder(\n",
    "            encoder_channels=encoder_channels,\n",
    "            decoder_channels=decoder_channels,\n",
    "            final_channels=num_classes,\n",
    "            norm_layer=norm_layer,\n",
    "            center=center,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.encoder(x)\n",
    "        x.reverse()  # torchscript doesn't work with [::-1]\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conv2dBnAct(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=0,\n",
    "                 stride=1, act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = norm_layer(out_channels)\n",
    "        self.act = act_layer(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, scale_factor=2.0, act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d):\n",
    "        super().__init__()\n",
    "        conv_args = dict(kernel_size=3, padding=1, act_layer=act_layer)\n",
    "        self.scale_factor = scale_factor\n",
    "        if norm_layer is None:\n",
    "            self.conv1 = Conv2dBnAct(in_channels, out_channels, **conv_args)\n",
    "            self.conv2 = Conv2dBnAct(out_channels, out_channels,  **conv_args)\n",
    "        else:\n",
    "            self.conv1 = Conv2dBnAct(in_channels, out_channels, norm_layer=norm_layer, **conv_args)\n",
    "            self.conv2 = Conv2dBnAct(out_channels, out_channels, norm_layer=norm_layer, **conv_args)\n",
    "\n",
    "    def forward(self, x, skip: Optional[torch.Tensor] = None):\n",
    "        if self.scale_factor != 1.0:\n",
    "            x = F.interpolate(x, scale_factor=self.scale_factor, mode='nearest')\n",
    "        if skip is not None:\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UnetDecoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            encoder_channels,\n",
    "            decoder_channels=(256, 128, 64, 32, 16),\n",
    "            final_channels=1,\n",
    "            norm_layer=nn.BatchNorm2d,\n",
    "            center=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if center:\n",
    "            channels = encoder_channels[0]\n",
    "            self.center = DecoderBlock(channels, channels, scale_factor=1.0, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.center = nn.Identity()\n",
    "\n",
    "        in_channels = [in_chs + skip_chs for in_chs, skip_chs in zip(\n",
    "            [encoder_channels[0]] + list(decoder_channels[:-1]),\n",
    "            list(encoder_channels[1:]) + [0])]\n",
    "        out_channels = decoder_channels\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for in_chs, out_chs in zip(in_channels, out_channels):\n",
    "            self.blocks.append(DecoderBlock(in_chs, out_chs, norm_layer=norm_layer))\n",
    "        self.final_conv = nn.Conv2d(out_channels[-1], final_channels, kernel_size=(1, 1))\n",
    "\n",
    "        self._init_weight()\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x: List[torch.Tensor]):\n",
    "        encoder_head = x[0]\n",
    "        skips = x[1:]\n",
    "        x = self.center(encoder_head)\n",
    "        for i, b in enumerate(self.blocks):\n",
    "            skip = skips[i] if i < len(skips) else None\n",
    "            x = b(x, skip)\n",
    "        x = self.final_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d299f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetBased(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        genes,\n",
    "        tile_resolution = 16,\n",
    "        n_metagenes = 20,\n",
    "        in_channels = 3,\n",
    "        out_channels = 64,\n",
    "        decoder_channels = (128, 64, 32, 16, 8),\n",
    "        context_decoder_channels = (128, 64, 32, 16, 8),\n",
    "        he_scaler = .1,\n",
    "        kl_scaler = .001,\n",
    "        exp_scaler = 1.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.genes = genes\n",
    "        self.n_genes = len(genes)\n",
    "\n",
    "        \n",
    "        self.he_scaler = he_scaler\n",
    "        self.kl_scaler = kl_scaler\n",
    "        self.exp_scaler = exp_scaler\n",
    "        \n",
    "        self.unet = Unet(backbone='resnet34',\n",
    "                         decoder_channels=decoder_channels,\n",
    "                         in_chans=in_channels,\n",
    "                         num_classes=out_channels)\n",
    "        \n",
    "        self.context_unet = Unet(backbone='resnet34',\n",
    "                         decoder_channels=context_decoder_channels,\n",
    "                         in_chans=in_channels,\n",
    "                         num_classes=out_channels)\n",
    "        \n",
    "        self.post_unet_conv = nn.Conv2d(in_channels=out_channels * 2, out_channels=out_channels,\n",
    "                                        kernel_size=1)\n",
    "\n",
    "        # latent mu and var\n",
    "        self.latent_mu = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=1)\n",
    "        self.latent_var = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=1)\n",
    "        self.latent_norm = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.n_metagenes = n_metagenes\n",
    "        self.tile_resolution = tile_resolution\n",
    "        self.metagenes = torch.nn.Parameter(torch.rand(self.n_metagenes, self.n_genes))\n",
    "        self.scale_factors = torch.nn.Parameter(torch.rand(self.n_genes))\n",
    "        self.p = torch.nn.Parameter(torch.rand(self.n_genes))\n",
    "        \n",
    "        self.post_decode_he = torch.nn.Conv2d(out_channels, 3, 1)\n",
    "        self.post_decode_exp = torch.nn.Conv2d(out_channels, self.n_metagenes, 1)\n",
    "        \n",
    "        self.he_loss = torch.nn.MSELoss()\n",
    "        \n",
    "    def _kl_divergence(self, z, mu, std):\n",
    "        # lightning imp.\n",
    "        # Monte carlo KL divergence\n",
    "        p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "\n",
    "        log_qzx = q.log_prob(z)\n",
    "        log_pz = p.log_prob(z)\n",
    "\n",
    "        kl = (log_qzx - log_pz)\n",
    "        kl = kl.sum(-1)\n",
    "\n",
    "        return kl\n",
    "\n",
    "    def encode(self, x, x_context, use_means=False):\n",
    "        x_encoded = self.unet(x)\n",
    "        x_context_encoded = self.context_unet(x)\n",
    "        \n",
    "        x_encoded = torch.concat((x_encoded, x_context_encoded), dim=1)\n",
    "        x_encoded = self.post_unet_conv(x_encoded)\n",
    "        \n",
    "        x_encoded = self.latent_norm(x_encoded)\n",
    "        \n",
    "        mu, log_var = self.latent_mu(x_encoded), self.latent_var(x_encoded)\n",
    "        \n",
    "        # sample z from parameterized distributions\n",
    "        std = torch.exp(log_var / 2)\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "        # get our latent\n",
    "        if use_means:\n",
    "            z = mu\n",
    "        else:\n",
    "            z = q.rsample()\n",
    "\n",
    "        return z, mu, std\n",
    "    \n",
    "    def calculate_loss(self, he_true, exp_true, result):\n",
    "        exp_loss = torch.mean(-result['nb'].log_prob(exp_true))\n",
    "        \n",
    "        kl_loss = torch.mean(self._kl_divergence(result['z'], result['z_mu'], result['z_std']))\n",
    "        \n",
    "        he_loss = torch.mean(self.he_loss(he_true, result['he']))\n",
    "        \n",
    "        return {\n",
    "            'overall_loss': exp_loss * self.exp_scaler + kl_loss * self.kl_scaler + he_loss * self.he_scaler,\n",
    "            'exp_loss': exp_loss,\n",
    "            'kl_loss': kl_loss,\n",
    "            'he_loss': he_loss\n",
    "        }\n",
    "    \n",
    "    def reconstruct_expression(self, dec, masks=None, voxel_idxs=None, reduce_to_voxel=True):\n",
    "        x = self.post_decode_exp(dec) # (b c h w)\n",
    "        \n",
    "        if reduce_to_voxel:\n",
    "            x = reduce_to_voxel_level(x, masks) # (b, v, m)\n",
    "        else:  \n",
    "            x = rearrange(x, 'b c h w -> b h w c')\n",
    "        \n",
    "        r = x @ self.metagenes\n",
    "        r = r * self.scale_factors\n",
    "        r = F.softplus(r)\n",
    "        \n",
    "        p = torch.sigmoid(self.p)\n",
    "        \n",
    "        if reduce_to_voxel:\n",
    "            p = rearrange(p, 'c -> 1 1 c')\n",
    "            r = mask_nb_params(r, voxel_idxs)\n",
    "        else:\n",
    "            p = rearrange(p, 'c -> 1 1 1 c')\n",
    "            \n",
    "        r += .00000001\n",
    "            \n",
    "        nb = torch.distributions.NegativeBinomial(r, p)\n",
    "        \n",
    "        return {\n",
    "            'r': r,\n",
    "            'p': p,\n",
    "            'exp': nb.mean,\n",
    "            'nb': nb,\n",
    "            'metagene_activity': x # (b v m)\n",
    "        }\n",
    "    \n",
    "    def reconstruct_he(self, dec):\n",
    "        he = self.post_decode_he(dec)\n",
    "        return he\n",
    "\n",
    "    def forward(self, x, x_context, masks=None, voxel_idxs=None, reduce_to_voxel=True, use_means=False):\n",
    "        z, z_mu, z_std = self.encode(x, x_context, use_means=use_means)\n",
    "\n",
    "        he = self.reconstruct_he(z)\n",
    "        \n",
    "        exp_result = self.reconstruct_expression(z, masks=masks, voxel_idxs=voxel_idxs,\n",
    "                                                 reduce_to_voxel=reduce_to_voxel)\n",
    "        \n",
    "        result = {\n",
    "            'z': z,\n",
    "            'z_mu': z_mu,\n",
    "            'z_std': z_std,\n",
    "            'he': he,\n",
    "        }\n",
    "        result.update(exp_result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262ae5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_to_voxel_level(x, masks):\n",
    "    \"\"\"\n",
    "    x - (b, c, h, w)\n",
    "    masks - (b, v, h, w)\n",
    "    \n",
    "    out - (b, v, c)\n",
    "    \"\"\"\n",
    "    masks = masks.unsqueeze(dim=2) # (b, v, c, h, w)\n",
    "    x = x.unsqueeze(dim=1).repeat(1, 16, 1, 1, 1) # (b, v, c, h, w)\n",
    "    x *= masks \n",
    "    return x.sum(dim=(-1, -2)) # (b, v, m)\n",
    "\n",
    "def mask_nb_params(r, voxel_idxs):\n",
    "    mask = torch.zeros_like(voxel_idxs, dtype=torch.bool)\n",
    "    if r.is_cuda:\n",
    "        mask = mask.cuda()\n",
    "        \n",
    "    mask[voxel_idxs == 0] = 1\n",
    "\n",
    "    mask = mask.unsqueeze(dim=-1)\n",
    "    masked_r = r.masked_fill(mask, 0.)\n",
    "    \n",
    "    return masked_r\n",
    "\n",
    "def construct_tile_expression(padded_exp, masks, n_voxels, normalize=True):\n",
    "    tile = torch.zeros((masks.shape[0], masks.shape[-2], masks.shape[-1], padded_exp.shape[-1]),\n",
    "                       device=padded_exp.device)\n",
    "    for b in range(tile.shape[0]):\n",
    "        for exp, m in zip(padded_exp[b], masks[b]):\n",
    "            tile[b, :, :][m==1] = exp.to(torch.float32)\n",
    "            \n",
    "    tile = rearrange(tile, 'b h w c -> b c h w')\n",
    "    tile = tile.detach().cpu().numpy()\n",
    "    \n",
    "    tile /= np.expand_dims(tile.max(axis=(0, -2, -1)), (0, -2, -1))\n",
    "\n",
    "    return rearrange(tile, 'b c h w -> b h w c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc53b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_intermediates(logger, batch, result, plot_genes, model,\n",
    "                      n_samples=8, result_full_res=None, identifier='train'):\n",
    "    model_genes = np.asarray(model.genes)\n",
    "    g2i = {g:i for i, g in enumerate(model_genes)}\n",
    "    gene_idxs = np.asarray([g2i[g] for g in plot_genes])\n",
    "    \n",
    "    img = batch['he_context'][:n_samples].clone().detach()\n",
    "    img -= img.min()\n",
    "    img /= img.max()\n",
    "    logger.log_image(\n",
    "        key=f\"{identifier}/he_context\",\n",
    "        images=[img],\n",
    "        caption=[f'{identifier} he context']\n",
    "    )\n",
    "    \n",
    "    img = batch['he'][:n_samples].clone().detach()\n",
    "    img -= img.min()\n",
    "    img /= img.max()\n",
    "    logger.log_image(\n",
    "        key=f\"{identifier}/he_groundtruth\",\n",
    "        images=[img],\n",
    "        caption=[f'{identifier} he tile 2x']\n",
    "    )\n",
    "    \n",
    "    img = result['he'][:n_samples].clone().detach() # (b c h w)\n",
    "    img -= img.min()\n",
    "    img /= img.max()\n",
    "    logger.log_image(\n",
    "        key=f\"{identifier}/he_reconstruction\",\n",
    "        images=[img],\n",
    "        caption=[f'{identifier} he tile recon']\n",
    "    )\n",
    "\n",
    "    recon = construct_tile_expression(batch['exp'], batch['masks'], batch['n_voxels'])\n",
    "    recon = recon[:n_samples, :, :, gene_idxs]\n",
    "    recon = torch.tensor(rearrange(recon, 'b h w c -> c b 1 h w'))\n",
    "    logger.log_image(\n",
    "        key=f\"{identifier}/exp_groundtruth\",\n",
    "        images=[img for img in recon],\n",
    "        caption=[g for g in plot_genes]\n",
    "    )\n",
    "\n",
    "    recon = construct_tile_expression(result['exp'], batch['masks'], batch['n_voxels'])\n",
    "    recon = recon[:n_samples, :, :, gene_idxs]\n",
    "    recon = torch.tensor(rearrange(recon, 'b h w c -> c b 1 h w'))\n",
    "    logger.log_image(\n",
    "        key=f\"{identifier}/exp_reconstruction\",\n",
    "        images=[img for img in recon],\n",
    "        caption=[g for g in plot_genes]\n",
    "    )\n",
    "    \n",
    "    recon = result['metagene_activity'][:n_samples, :, gene_idxs].clone().detach().to(torch.float32)\n",
    "    recon -= recon.min()\n",
    "    recon /= recon.max()\n",
    "    logger.log_image(\n",
    "        key=f\"{identifier}/metagene_activity\",\n",
    "        images=[img for img in recon],\n",
    "    )\n",
    "    \n",
    "    vals = model.metagenes.clone().detach().cpu().numpy()\n",
    "    vals = vals[:, gene_idxs]\n",
    "    df = pd.DataFrame(data=vals, columns=plot_genes)\n",
    "    logger.log_text(\n",
    "        key=f'{identifier}/metagenes',\n",
    "        dataframe=df\n",
    "    )\n",
    "    \n",
    "    vals = model.scale_factors.clone().detach().cpu().numpy()\n",
    "    vals = vals[gene_idxs]\n",
    "    df = pd.DataFrame(data=[vals], columns=plot_genes)\n",
    "    logger.log_text(\n",
    "        key=f'{identifier}/scale_factors',\n",
    "        dataframe=df\n",
    "    )\n",
    "    \n",
    "    if result_full_res is not None:\n",
    "        recon = result_full_res['exp'][:n_samples].clone().detach()\n",
    "        recon = recon[:, :, :, gene_idxs]\n",
    "        recon = rearrange(recon, 'b h w c -> c b 1 h w')\n",
    "        logger.log_image(\n",
    "            key=f\"{identifier}/full_res_exp_reconstruction\",\n",
    "            images=[img for img in recon],\n",
    "            caption=[g for g in plot_genes]\n",
    "        )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e71eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class xFuseLightning(pl.LightningModule):\n",
    "    def __init__(self, autokl, lr=1e-4, n_samples=8, plot_genes=['IL7R', 'KRT18', 'BGN', 'PECAM1', 'INS'],\n",
    "                 train_epoch_fraction=.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.autokl = autokl\n",
    "        self.lr = lr\n",
    "        self.plot_genes = plot_genes\n",
    "        self.n_samples = n_samples\n",
    "        self.train_epoch_fraction = train_epoch_fraction\n",
    "        \n",
    "        self.save_hyperparameters(ignore=['autokl'])\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, x_context, masks, voxel_idxs, exp = batch['he'], batch['he_context'], batch['masks'], batch['voxel_idxs'], batch['exp']\n",
    "        result = self.autokl(x, x_context, masks=masks, voxel_idxs=voxel_idxs)\n",
    "        losses = self.autokl.calculate_loss(x, exp, result)\n",
    "        losses = {f'train/{k}':v for k, v in losses.items()}\n",
    "        self.log_dict(losses, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        losses['loss'] = losses['train/overall_loss']\n",
    "        \n",
    "        # only log 10-ish% of training epochs\n",
    "        if batch_idx == 0 and torch.rand(1).item() < self.train_epoch_fraction:\n",
    "            result_full_res = self.autokl(x[:1], x_context[:1], reduce_to_voxel=False)\n",
    "            log_intermediates(self.logger, batch, result, self.plot_genes, self.autokl,\n",
    "                              n_samples=self.n_samples, result_full_res=result_full_res, identifier='train')\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, x_context, masks, voxel_idxs, exp = batch['he'], batch['he_context'], batch['masks'], batch['voxel_idxs'], batch['exp']\n",
    "        result = self.autokl(x, x_context, masks=masks, voxel_idxs=voxel_idxs)\n",
    "        losses = self.autokl.calculate_loss(x, exp, result)\n",
    "        losses = {f'val/{k}':v for k, v in losses.items()}\n",
    "        self.log_dict(losses, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            result_full_res = self.autokl(x[:1], x_context[:1], reduce_to_voxel=False)\n",
    "            log_intermediates(self.logger, batch, result, self.plot_genes, self.autokl,\n",
    "                              n_samples=self.n_samples, result_full_res=result_full_res, identifier='val')\n",
    "        \n",
    "        return losses\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f085f0d9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### test forwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc6928",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_dl))\n",
    "x, b, masks, voxel_idxs, exp, exp_tiles = batch['he'], batch['b'], batch['masks'], batch['voxel_idxs'], batch['exp'], batch['exp_tiles']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e1360b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# autokl = AutoencoderKL(\n",
    "#     train_adata.shape[1],\n",
    "#     tile_resolution=32,\n",
    "#     n_metagenes=20,\n",
    "#     in_channels=3,\n",
    "#     out_channels=64,\n",
    "#     down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\", \"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n",
    "#     up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\", \"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n",
    "#     block_out_channels=[8, 16, 32, 64],\n",
    "#     norm_num_groups=8,\n",
    "#     latent_channels=4,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87370a52",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "autokl = UnetBased(\n",
    "    train_adata.var.index.to_list(),\n",
    "    tile_resolution=32,\n",
    "    n_metagenes=20,\n",
    "    in_channels=3,\n",
    "    out_channels=64,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e145a917",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result = autokl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e481b0b8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5339d5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51edb14e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1716e7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result['exp'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18204c7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(result['exp'][0, :, :, 5].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8922998",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(result['metagene_activity'][0, :, :, 0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0802b2e9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "losses = autokl.calculate_loss(x, exp_tiles, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da29d920",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412f6363",
   "metadata": {},
   "source": [
    "#### training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b860e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'unet_based'\n",
    "log_dir = '/scratch1/fs1/dinglab/estorrs/deep-spatial-genomics/logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0237ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "logger = WandbLogger(project=project, save_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8d21d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805e75b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'n_genes': next(iter(sample_to_train_adata.values())).shape[1],\n",
    "    'genes': next(iter(sample_to_train_adata.values())).var.index.to_numpy(),\n",
    "    'plot_genes': ['IL7R', 'BGN', 'KRT18', 'PECAM1', 'INS'],\n",
    "    'n_covariates': 1,\n",
    "    'n_metagenes': 10,\n",
    "    'latent_dim': 64,\n",
    "    'tile_resolution': 32,\n",
    "    'he_scale': '2X',\n",
    "    'he_context_scale': '8X',\n",
    "    'encoder': {\n",
    "        'model': 'unet',\n",
    "        'in_channels': 3,\n",
    "        'decoder_channels': (128, 64, 32, 16, 8),\n",
    "        'context_decoder_channels': (128, 64, 32, 16, 8),\n",
    "    },\n",
    "    'kl_scalers': {\n",
    "        'exp_scaler': 1.,\n",
    "        'kl_scaler': .0001,\n",
    "        'he_scaler': .1,\n",
    "    },\n",
    "    'training': {\n",
    "        'train_samples': list(sample_to_train_adata.keys()),\n",
    "        'val_samples': list(sample_to_val_adata.keys()),\n",
    "        'log_n_samples': 8,\n",
    "        'max_epochs': 1000,\n",
    "        'check_val_every_n_epoch': 10,\n",
    "        'log_train_fraction': 1.,\n",
    "        'log_every_n_steps': 1,\n",
    "        'accelerator': 'gpu',\n",
    "        'devices': 1,\n",
    "        'limit_train_batches': 1.,\n",
    "        'limit_val_batches': .1,\n",
    "        'lr': 1e-4,\n",
    "        'batch_size': batch_size,\n",
    "        'precision': 32\n",
    "    },\n",
    "}\n",
    "logger.experiment.config.update(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f8cca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "autokl = UnetBased(\n",
    "    config['genes'],\n",
    "    tile_resolution=config['tile_resolution'],\n",
    "    n_metagenes=config['n_metagenes'],\n",
    "    in_channels=config['encoder']['in_channels'],\n",
    "    out_channels=config['latent_dim'],\n",
    "    decoder_channels=config['encoder']['decoder_channels'],\n",
    "    context_decoder_channels=config['encoder']['context_decoder_channels'],\n",
    "    he_scaler=config['kl_scalers']['he_scaler'],\n",
    "    kl_scaler=config['kl_scalers']['kl_scaler'],\n",
    "    exp_scaler=config['kl_scalers']['exp_scaler'],\n",
    ")\n",
    "model = xFuseLightning(autokl, lr=config['training']['lr'],\n",
    "                       n_samples=config['training']['log_n_samples'],\n",
    "                       train_epoch_fraction=config['training']['log_train_fraction'],\n",
    "                       plot_genes=config['plot_genes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816cdca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    devices=config['training']['devices'],\n",
    "    accelerator=config['training']['accelerator'],\n",
    "    check_val_every_n_epoch=config['training']['check_val_every_n_epoch'],\n",
    "    enable_checkpointing=False,\n",
    "    limit_val_batches=config['training']['limit_val_batches'],\n",
    "    limit_train_batches=config['training']['limit_train_batches'],\n",
    "    log_every_n_steps=config['training']['log_every_n_steps'],\n",
    "    max_epochs=config['training']['max_epochs'],\n",
    "    precision=config['training']['precision'],\n",
    "    logger=logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430a05b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.fit(model=model, train_dataloaders=train_dl, val_dataloaders=val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769f4587",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /scratch1/fs1/dinglab/estorrs/deep-spatial-genomics/runs/xfuse_improved_v5/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d95f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/scratch1/fs1/dinglab/estorrs/deep-spatial-genomics/runs/xfuse_improved_v5/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17476978",
   "metadata": {},
   "outputs": [],
   "source": [
    "autokl = UnetBased(\n",
    "    config['genes'],\n",
    "    tile_resolution=config['tile_resolution'],\n",
    "    n_metagenes=config['n_metagenes'],\n",
    "    in_channels=config['encoder']['in_channels'],\n",
    "    out_channels=config['latent_dim'],\n",
    "    decoder_channels=config['encoder']['decoder_channels'],\n",
    "    context_decoder_channels=config['encoder']['context_decoder_channels'],\n",
    "    he_scaler=config['kl_scalers']['he_scaler'],\n",
    "    kl_scaler=config['kl_scalers']['kl_scaler'],\n",
    "    exp_scaler=config['kl_scalers']['exp_scaler'],\n",
    ")\n",
    "model = xFuseLightning(autokl, lr=config['training']['lr'],\n",
    "                       n_samples=config['training']['log_n_samples'],\n",
    "                       train_epoch_fraction=config['training']['log_train_fraction'])\n",
    "model.load_state_dict(torch.load('/scratch1/fs1/dinglab/estorrs/deep-spatial-genomics/runs/xfuse_improved_v5/model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ced218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = next(iter(sample_to_train_adata.values()))\n",
    "# a = sample_to_val_adata['HT264P1-S1H2U1']\n",
    "a = sample_to_train_adata['HT270P1-S1H1U1']\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc71724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# he = a.uns['rescaled_he']['2X_notrim']\n",
    "he = a.uns['rescaled_he']['2X_trimmed']\n",
    "\n",
    "he.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc485c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(he)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25194ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context_he = a.uns['rescaled_he']['8X_notrim']\n",
    "context_he = a.uns['rescaled_he']['8X_trimmed']\n",
    "context_he.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b1fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(context_he)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c01d4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflection_mosiac(x, border=256):\n",
    "    max_r, max_c = x.shape[-2], x.shape[-1]\n",
    "    if len(x.shape) == 3:\n",
    "        mosaic = torch.zeros((x.shape[0], max_r + (border * 2), max_c + (border * 2))).to(x.dtype)\n",
    "    else:\n",
    "        mosaic = torch.zeros((max_r + (border * 2), max_c + (border * 2))).to(x.dtype)\n",
    "    \n",
    "    # make tiles\n",
    "    top_left = TF.pad(x, padding=[border, border, 0, 0], padding_mode='reflect')\n",
    "    top_right = TF.pad(x, padding=[0, border, border, 0], padding_mode='reflect')\n",
    "    bottom_left = TF.pad(x, padding=[border, 0, 0, border], padding_mode='reflect')\n",
    "    bottom_right = TF.pad(x, padding=[0, 0, border, border], padding_mode='reflect')\n",
    "    \n",
    "    if len(x.shape) == 3:\n",
    "        mosaic[:, :max_r + border, :max_c + border] = top_left\n",
    "        mosaic[:, :max_r + border, border:] = top_right\n",
    "        mosaic[:, border:, :max_c + border] = bottom_left\n",
    "        mosaic[:, border:, border:] = bottom_right\n",
    "    else:\n",
    "        mosaic[:max_r + border, :max_c + border] = top_left\n",
    "        mosaic[:max_r + border, border:] = top_right\n",
    "        mosaic[border:, :max_c + border] = bottom_left\n",
    "        mosaic[border:, border:] = bottom_right\n",
    "    \n",
    "    return mosaic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58545c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_img(img, context_img, context_scale, tile_size=256, window_scale=2):\n",
    "    context_img = reflection_mosiac(context_img, border=tile_size)\n",
    "    \n",
    "    n_rows = (img.shape[1] // tile_size) * window_scale + 1\n",
    "    n_cols = (img.shape[2] // tile_size) * window_scale + 1\n",
    "    \n",
    "    tiles = torch.ones(n_rows * n_cols, img.shape[0], tile_size, tile_size, dtype=img.dtype)\n",
    "    expanded = torch.ones(img.shape[0],\n",
    "                          n_rows * tile_size // window_scale + tile_size,\n",
    "                          n_cols * tile_size // window_scale + tile_size,\n",
    "                          dtype=img.dtype)\n",
    "    expanded[:, :img.shape[1], :img.shape[2]] = img\n",
    "    \n",
    "    context_tiles = torch.ones(n_rows * n_cols, img.shape[0], tile_size, tile_size, dtype=context_img.dtype)\n",
    "    \n",
    "    idx = 0\n",
    "    top_left = []\n",
    "    for r in range(n_rows):\n",
    "        for c in range(n_cols):\n",
    "            r1 = r * tile_size // window_scale\n",
    "            c1 = c * tile_size // window_scale\n",
    "            r2 = r1 + tile_size\n",
    "            c2 = c1 + tile_size\n",
    "            tiles[idx] = expanded[:, r1:r2, c1:c2]\n",
    "            top_left.append((r, c))\n",
    "            \n",
    "            center_r = r1 + (tile_size / 2)\n",
    "            center_c = c1 + (tile_size / 2)\n",
    "            center_r = int(center_r / context_scale) \n",
    "            center_c = int(center_c / context_scale) \n",
    "            r1 = center_r - tile_size // 2 + tile_size\n",
    "            c1 = center_c - tile_size // 2 + tile_size\n",
    "            r2 = r1 + tile_size\n",
    "            c2 = c1 + tile_size\n",
    "            context_tiles[idx] = context_img[:, r1:r2, c1:c2]\n",
    "            idx += 1\n",
    "    return tiles, context_tiles, top_left\n",
    "\n",
    "def get_tile(r, c, img, context_img, context_scale, tile_size=256, window_scale=2):\n",
    "    r1 = r * tile_size // window_scale\n",
    "    c1 = c * tile_size // window_scale\n",
    "    r2 = r1 + tile_size\n",
    "    c2 = c1 + tile_size\n",
    "    tile = img[:, r1:r2, c1:c2]\n",
    "\n",
    "    center_r = r1 + (tile_size / 2)\n",
    "    center_c = c1 + (tile_size / 2)\n",
    "    center_r = int(center_r / context_scale) \n",
    "    center_c = int(center_c / context_scale) \n",
    "    r1 = center_r - tile_size // 2 + tile_size\n",
    "    c1 = center_c - tile_size // 2 + tile_size\n",
    "    r2 = r1 + tile_size\n",
    "    c2 = c1 + tile_size\n",
    "    context_tile = context_img[:, r1:r2, c1:c2]\n",
    "    \n",
    "    return tile, context_tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5411506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_img(img):\n",
    "    if img.shape[0] != 3:\n",
    "        img = rearrange(img, 'h w c -> c h w')\n",
    "    if not isinstance(img, torch.Tensor):\n",
    "        img = torch.tensor(img)\n",
    "    if not img.dtype == torch.float32:\n",
    "        img = TF.convert_image_dtype(img, dtype=torch.float32)\n",
    "    \n",
    "    return img \n",
    "\n",
    "class TiledHEDatasetV2(Dataset):\n",
    "    def __init__(self, img, context_img, context_scale, normalize=True, tile_size=256, window_scale=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        img = prepare_img(img)\n",
    "        context_img = prepare_img(context_img)\n",
    "\n",
    "        self.context_scale = context_scale\n",
    "        self.tile_size = tile_size\n",
    "        self.window_scale = window_scale\n",
    "        \n",
    "        if normalize:\n",
    "            self.normalize = Normalize((0.771, 0.651, 0.752), (0.229, 0.288, 0.224)) # from HT397B1-H2 ffpe H&E image\n",
    "        else:\n",
    "            self.normalize = nn.Identity()\n",
    "            \n",
    "        self.context_img = reflection_mosiac(context_img, border=tile_size)\n",
    "    \n",
    "        self.n_rows = (img.shape[1] // tile_size) * window_scale + 2\n",
    "        self.n_cols = (img.shape[2] // tile_size) * window_scale + 2\n",
    "        \n",
    "        self.img = torch.ones(img.shape[0],\n",
    "                          self.n_rows * tile_size // window_scale + tile_size,\n",
    "                          self.n_cols * tile_size // window_scale + tile_size,\n",
    "                          dtype=img.dtype)\n",
    "        self.img[:, :img.shape[1], :img.shape[2]] = img\n",
    "        \n",
    "        self.idx_to_top_left = [(r, c)\n",
    "                                for r in range(self.n_rows)\n",
    "                                for c in range(self.n_cols)]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_top_left)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        r, c = self.idx_to_top_left[idx]\n",
    "        \n",
    "        tile, context_tile = get_tile(r, c, self.img, self.context_img, self.context_scale,\n",
    "                                     tile_size=self.tile_size, window_scale=self.window_scale)\n",
    "        return {\n",
    "            'he': self.normalize(tile),\n",
    "            'context_he': self.normalize(context_tile),\n",
    "            'top': r,\n",
    "            'left': c\n",
    "        }\n",
    "    \n",
    "class TiledHEDataset(Dataset):\n",
    "    def __init__(self, img, context_img, context_scale, normalize=True, tile_size=256, window_scale=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        img = prepare_img(img)\n",
    "        context_img = prepare_img(context_img)\n",
    "\n",
    "        self.img = img\n",
    "        self.context_img = context_img\n",
    "        self.context_scale = context_scale\n",
    "        self.tiles, self.context_tiles, self.idx_to_top_left = tile_img(img, context_img, context_scale,\n",
    "                                                                        tile_size=tile_size, window_scale=window_scale)\n",
    "        \n",
    "        if normalize:\n",
    "            self.normalize = Normalize((0.771, 0.651, 0.752), (0.229, 0.288, 0.224)) # from HT397B1-H2 ffpe H&E image\n",
    "        else:\n",
    "            self.normalize = nn.Identity()\n",
    "            \n",
    "    def max_r(self):\n",
    "        rs, cs = zip(*self.idx_to_top_left)\n",
    "        return np.max(rs)\n",
    "\n",
    "    def max_c(self):\n",
    "        rs, cs = zip(*self.idx_to_top_left)\n",
    "        return np.max(cs)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.tiles.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tile, context_tile, (r, c) = self.tiles[idx], self.context_tiles[idx], self.idx_to_top_left[idx]\n",
    "        return {\n",
    "            'he': self.normalize(tile),\n",
    "            'context_he': self.normalize(context_tile),\n",
    "            'top': r,\n",
    "            'left': c\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f78e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tile(tile, new, r, c, max_r, max_c, window_scale=2,):\n",
    "    ts = tile.shape[0] // window_scale\n",
    "    normal_offset = ts // 2\n",
    "    edge_offset = tile.shape[0] // 2\n",
    "    rc, cc = r * ts + edge_offset, c * ts + edge_offset\n",
    "    trc, tcc = edge_offset, edge_offset\n",
    "\n",
    "    r1 = rc - normal_offset\n",
    "    r2 = rc + normal_offset\n",
    "    c1 = cc - normal_offset\n",
    "    c2 = cc + normal_offset\n",
    "    tr1 = trc - normal_offset\n",
    "    tr2 = trc + normal_offset\n",
    "    tc1 = tcc - normal_offset\n",
    "    tc2 = tcc + normal_offset\n",
    "    if r == 0:\n",
    "        r1 = rc - edge_offset\n",
    "        r2 = rc + normal_offset\n",
    "        tr1 = trc - edge_offset\n",
    "        tr2 = trc + normal_offset\n",
    "    if c == 0:\n",
    "        c1 = cc - edge_offset\n",
    "        c2 = cc + normal_offset\n",
    "        tc1 = tcc - edge_offset\n",
    "        tc2 = tcc + normal_offset\n",
    "    if r == max_r:\n",
    "        r1 = rc - normal_offset\n",
    "        r2 = rc + edge_offset\n",
    "        tr1 = trc - normal_offset\n",
    "        tr2 = trc + edge_offset\n",
    "    if c == max_c:\n",
    "        c1 = cc - normal_offset\n",
    "        c2 = cc + edge_offset\n",
    "        tc1 = tcc - normal_offset\n",
    "        tc2 = tcc + edge_offset\n",
    "\n",
    "    new[r1:r2, c1:c2] = tile[tr1:tr2, tc1:tc2]\n",
    "\n",
    "def predict_he(img, context_img, context_scale, model,\n",
    "               batch_size=8,\n",
    "               gene_idxs=None, keep=('he', 'exp', 'metagene_activity'),\n",
    "               window_scale=2, rescale=False, resize=True, crop=True):\n",
    "    orig_shape = img.shape\n",
    "    ds = TiledHEDatasetV2(img, context_img, context_scale, window_scale=window_scale)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    if gene_idxs is None:\n",
    "        gene_idxs = np.arange(len(model.autokl.genes))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(dl))\n",
    "        he, he_context, top, left = batch['he'], batch['context_he'], batch['top'], batch['left']\n",
    "\n",
    "        if next(iter(model.autokl.parameters())).is_cuda:\n",
    "            he, he_context = he.cuda(), he_context.cuda()\n",
    "\n",
    "        result = model.autokl(he, context_he, reduce_to_voxel=False)\n",
    "        item = {k:v[0].detach().cpu()\n",
    "               for k, v in result.items()\n",
    "               if k in keep}\n",
    "        item['exp'] = item['exp'][:, :, gene_idxs]\n",
    "        item['he'] = rearrange(item['he'], 'c h w -> h w c')\n",
    "    max_r, max_c = ds.n_rows, ds.n_cols\n",
    "    key_to_new = {k:torch.ones(tile.shape[0] * (max_r + 2) // window_scale,\n",
    "                               tile.shape[1] * (max_c + 2) // window_scale,\n",
    "                               tile.shape[2],\n",
    "                               dtype=tile.dtype)\n",
    "                 for k, tile in item.items()}\n",
    "#     return item\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dl:\n",
    "            he, he_context, top, left = batch['he'], batch['context_he'], batch['top'], batch['left']\n",
    "\n",
    "            if next(iter(model.autokl.parameters())).is_cuda:\n",
    "                he, he_context = he.cuda(), he_context.cuda()\n",
    "\n",
    "            result = model.autokl(he, context_he, reduce_to_voxel=False)\n",
    "\n",
    "            for i in range(result['exp'].shape[0]):\n",
    "                item = {k:v[i].detach().cpu()\n",
    "                       for k, v in result.items()\n",
    "                       if k in keep}\n",
    "                item['exp'] = item['exp'][:, :, gene_idxs]\n",
    "                item['he'] = rearrange(item['he'], 'c h w -> h w c')\n",
    "                r, c = top[i], left[i]\n",
    "                for k in keep:\n",
    "                    tile = item[k]\n",
    "                    add_tile(tile, key_to_new[k], r, c, max_r, max_c, window_scale=window_scale)\n",
    "    \n",
    "    if resize:\n",
    "        for k, new in key_to_new.items():\n",
    "            new = new[:orig_shape[0], :orig_shape[1]]\n",
    "            key_to_new[k] = new\n",
    "    \n",
    "    if rescale:\n",
    "        for k, new in key_to_new.items():\n",
    "            if k == 'he':\n",
    "                new -= new.min()\n",
    "                new /= new.max()\n",
    "            else:\n",
    "                new -= new.amin(dim=(0, 1))\n",
    "                new /= new.amax(dim=(0, 1))\n",
    "            key_to_new[k] = new\n",
    "\n",
    "    return key_to_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa201875",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8648f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_genes = ['EPCAM', 'KRT18', 'CD8A', 'IL7R', 'MS4A1', 'PECAM1', 'BGN']\n",
    "# show_genes = ['KRT18', 'IL7R', 'PECAM1', 'BGN']\n",
    "show_genes = model.autokl.genes\n",
    "gene_idxs = [i for i, g in enumerate(model.autokl.genes) if g in show_genes]\n",
    "show_genes = model.autokl.genes[gene_idxs]\n",
    "key_to_retiled = predict_he(he, context_he, 4, model, gene_idxs=gene_idxs, rescale=True, window_scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a8fb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "he.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5083aa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, img in key_to_retiled.items():\n",
    "#     key_to_retiled[k] = img[:he.shape[0], :he.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6818c2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, img in key_to_retiled.items():\n",
    "    print(k, img.shape, img.max(), img.amax(dim=(0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d77c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(key_to_retiled['he'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11babd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(he)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1272a5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(key_to_retiled['exp'][:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1c3b12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, g in enumerate(show_genes):\n",
    "    plt.imshow(key_to_retiled['exp'][:, :, i])\n",
    "    plt.title(g)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f798948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(key_to_retiled['metagene_activity'].shape[-1]):\n",
    "    plt.imshow(key_to_retiled['metagene_activity'][:, :, i])\n",
    "    plt.title(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb51f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb3678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_to_retiled = predict_he(he, context_he, 4, model, gene_idxs=gene_idxs,\n",
    "#                             reduce_to_tile=False, batch_size=2, rescale=True, window_scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db45af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i, g in enumerate(show_genes):\n",
    "#     plt.imshow(np.log1p(key_to_retiled['exp'][:, :, i]))\n",
    "#     plt.title(g)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7093922",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cr1, cr2 = 3500, 4500\n",
    "cc1, cc2 = 4000, 5000\n",
    "for i, g in enumerate(show_genes):\n",
    "    plt.imshow(key_to_retiled['exp'][cr1:cr2, cc1:cc2, i])\n",
    "    plt.title(g)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c966494",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(he[cr1:cr2, cc1:cc2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9695015",
   "metadata": {},
   "outputs": [],
   "source": [
    "cr1, cr2 = 4000, 6000\n",
    "cc1, cc2 = 4000, 6000\n",
    "for i, g in enumerate(show_genes):\n",
    "    plt.imshow(key_to_retiled['exp'][cr1:cr2, cc1:cc2, i])\n",
    "    plt.title(g)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c185d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(he[cr1:cr2, cc1:cc2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a99ccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_mask = a.uns['rescaled_spot_masks']['2X_trimmed']\n",
    "spot_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de71219",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(spot_mask[cr1:cr2, cc1:cc2]>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de981dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_to_retiled['exp'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7146211",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save gene expression image\n",
    "exp = TF.convert_image_dtype(key_to_retiled['exp'], torch.uint8)\n",
    "exp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17c751d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(exp, '../data/annotations/pdac/HT270P1-S1H1U1_exp.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff58951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save gene expression image\n",
    "meta = TF.convert_image_dtype(key_to_retiled['metagene_activity'], torch.uint8)\n",
    "meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9604dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(meta, '../data/annotations/pdac/HT270P1-S1H1U1_meta.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dd3989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0199fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2333ee78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddd29fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29287f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9281da77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "metagenes = model.autokl.metagenes.detach().cpu().numpy()\n",
    "metagenes = pd.DataFrame(data=metagenes, columns=model.autokl.genes, index=np.arange(metagenes.shape[0]))\n",
    "metagenes = metagenes.loc[:, show_genes]\n",
    "sns.clustermap(data=metagenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eb4e22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56511ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aec214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee864e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d2937c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2525f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "388ae9cb",
   "metadata": {},
   "source": [
    "#### apply directly to H&E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ff14c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae7a5dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm import create_model\n",
    "from einops import rearrange, reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4ae2e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_spatial_genomics.models.x_unet import XUnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9ef02e",
   "metadata": {},
   "source": [
    "#### test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cee0d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 3561 × 17943\n",
       "    obs: 'in_tissue', 'array_row', 'array_col', 'clusters', 'alla_label'\n",
       "    var: 'gene_ids', 'feature_types', 'genome', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'\n",
       "    uns: 'alla_label_colors', 'full_res_he', 'hvg', 'leiden', 'log1p', 'neighbors', 'pca', 'spatial', 'umap'\n",
       "    obsm: 'X_pca', 'X_umap', 'spatial'\n",
       "    varm: 'PCs'\n",
       "    layers: 'counts'\n",
       "    obsp: 'connectivities', 'distances'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.read_h5ad('/storage1/fs1/dinglab/Active/Projects/estorrs/deep-spatial-genomics/data/brca/HT397B1-S1H2A4/adata.h5ad')\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369938cd",
   "metadata": {},
   "source": [
    "#### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ca5c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "modified from https://gist.github.com/rwightman/f8b24f4e6f5504aba03e999e02460d31\n",
    "\"\"\"\n",
    "class Unet(nn.Module):\n",
    "    \"\"\"Unet is a fully convolution neural network for image semantic segmentation\n",
    "    Args:\n",
    "        encoder_name: name of classification model (without last dense layers) used as feature\n",
    "            extractor to build segmentation model.\n",
    "        encoder_weights: one of ``None`` (random initialization), ``imagenet`` (pre-training on ImageNet).\n",
    "        decoder_channels: list of numbers of ``Conv2D`` layer filters in decoder blocks\n",
    "        decoder_use_batchnorm: if ``True``, ``BatchNormalisation`` layer between ``Conv2D`` and ``Activation`` layers\n",
    "            is used.\n",
    "        num_classes: a number of classes for output (output shape - ``(batch, classes, h, w)``).\n",
    "        center: if ``True`` add ``Conv2dReLU`` block on encoder head\n",
    "    NOTE: This is based off an old version of Unet in https://github.com/qubvel/segmentation_models.pytorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            backbone='resnet50',\n",
    "            backbone_kwargs=None,\n",
    "            backbone_indices=None,\n",
    "            decoder_use_batchnorm=True,\n",
    "            decoder_channels=(256, 128, 64, 32, 16),\n",
    "            in_chans=1,\n",
    "            num_classes=5,\n",
    "            center=False,\n",
    "            norm_layer=nn.BatchNorm2d,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        backbone_kwargs = backbone_kwargs or {}\n",
    "        # NOTE some models need different backbone indices specified based on the alignment of features\n",
    "        # and some models won't have a full enough range of feature strides to work properly.\n",
    "        encoder = create_model(\n",
    "            backbone, features_only=True, out_indices=backbone_indices, in_chans=in_chans,\n",
    "            pretrained=True, **backbone_kwargs)\n",
    "        encoder_channels = encoder.feature_info.channels()[::-1]\n",
    "        self.encoder = encoder\n",
    "\n",
    "        if not decoder_use_batchnorm:\n",
    "            norm_layer = None\n",
    "        self.decoder = UnetDecoder(\n",
    "            encoder_channels=encoder_channels,\n",
    "            decoder_channels=decoder_channels,\n",
    "            final_channels=num_classes,\n",
    "            norm_layer=norm_layer,\n",
    "            center=center,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.encoder(x)\n",
    "        x.reverse()  # torchscript doesn't work with [::-1]\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conv2dBnAct(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=0,\n",
    "                 stride=1, act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = norm_layer(out_channels)\n",
    "        self.act = act_layer(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, scale_factor=2.0, act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d):\n",
    "        super().__init__()\n",
    "        conv_args = dict(kernel_size=3, padding=1, act_layer=act_layer)\n",
    "        self.scale_factor = scale_factor\n",
    "        if norm_layer is None:\n",
    "            self.conv1 = Conv2dBnAct(in_channels, out_channels, **conv_args)\n",
    "            self.conv2 = Conv2dBnAct(out_channels, out_channels,  **conv_args)\n",
    "        else:\n",
    "            self.conv1 = Conv2dBnAct(in_channels, out_channels, norm_layer=norm_layer, **conv_args)\n",
    "            self.conv2 = Conv2dBnAct(out_channels, out_channels, norm_layer=norm_layer, **conv_args)\n",
    "\n",
    "    def forward(self, x, skip: Optional[torch.Tensor] = None):\n",
    "        if self.scale_factor != 1.0:\n",
    "            x = F.interpolate(x, scale_factor=self.scale_factor, mode='nearest')\n",
    "        if skip is not None:\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UnetDecoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            encoder_channels,\n",
    "            decoder_channels=(256, 128, 64, 32, 16),\n",
    "            final_channels=1,\n",
    "            norm_layer=nn.BatchNorm2d,\n",
    "            center=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if center:\n",
    "            channels = encoder_channels[0]\n",
    "            self.center = DecoderBlock(channels, channels, scale_factor=1.0, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.center = nn.Identity()\n",
    "\n",
    "        in_channels = [in_chs + skip_chs for in_chs, skip_chs in zip(\n",
    "            [encoder_channels[0]] + list(decoder_channels[:-1]),\n",
    "            list(encoder_channels[1:]) + [0])]\n",
    "        out_channels = decoder_channels\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for in_chs, out_chs in zip(in_channels, out_channels):\n",
    "            self.blocks.append(DecoderBlock(in_chs, out_chs, norm_layer=norm_layer))\n",
    "        self.final_conv = nn.Conv2d(out_channels[-1], final_channels, kernel_size=(1, 1))\n",
    "\n",
    "        self._init_weight()\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x: List[torch.Tensor]):\n",
    "        encoder_head = x[0]\n",
    "        skips = x[1:]\n",
    "        x = self.center(encoder_head)\n",
    "        for i, b in enumerate(self.blocks):\n",
    "            skip = skips[i] if i < len(skips) else None\n",
    "            x = b(x, skip)\n",
    "        x = self.final_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eba992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class xFuse(nn.Module):\n",
    "    def __init__(self, n_genes, voxel_size, n_covariates,\n",
    "                 n_metagenes=20, latent_dim=64, in_channels=3, decoder_channels=(64, 32, 16),\n",
    "                 kl_scaler = .001, latent_scaler=1., L_scaler=1., E_scaler=1., F_scaler=1.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.unet = Unet(backbone='resnet50',\n",
    "                        decoder_channels=(64, 32, 16),\n",
    "                        in_chans=in_channels,\n",
    "                        num_classes=latent_dim)\n",
    "        \n",
    "        self.n_genes = n_genes\n",
    "        self.voxel_size = voxel_size\n",
    "        self.n_metagenes = n_metagenes\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # scalers for KL loss on q for latent, L, E, and F\n",
    "        self.latent_scaler = latent_scaler\n",
    "        self.L_scaler = L_scaler\n",
    "        self.E_scaler = E_scaler\n",
    "        self.F_scaler = F_scaler\n",
    "        self.kl_scaler = kl_scaler\n",
    "        \n",
    "        # gene-wise baselines for r and p\n",
    "        self.t = nn.Parameter(torch.randn(self.n_genes))\n",
    "        self.u = nn.Parameter(torch.randn(self.n_genes))\n",
    "        \n",
    "        # latent mu and var\n",
    "        self.latent_mu = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=1)\n",
    "        self.latent_var = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=1)\n",
    "        \n",
    "        # creates initial metagene matrix - (b, H, W, m)\n",
    "        self.h = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=latent_dim, out_channels=self.n_metagenes, kernel_size=1),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # creates scale factor for each pixel - (1, H, W)\n",
    "        self.s = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=latent_dim, out_channels=1, kernel_size=1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        \n",
    "        # gene activity matrix - (m, g)\n",
    "        self.L = nn.Parameter(torch.randn(self.n_metagenes, self.n_genes))\n",
    "        self.L_mu = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=1)\n",
    "        self.L_var = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=1)\n",
    "        \n",
    "        # E - (D, g) - covariate matrix for r\n",
    "        self.E = nn.Parameter(torch.randn(self.n_covariates, self.n_genes))\n",
    "        self.E_mu = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=1)\n",
    "        self.E_var = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=1)\n",
    "        \n",
    "        # F - (D, g) - covariate matrix for p\n",
    "        self.F = nn.Parameter(torch.randn(self.n_covariates, self.n_genes))\n",
    "        self.F_mu = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=1)\n",
    "        self.F_var = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=1)\n",
    "        \n",
    "        self.reconstruction_loss = nn.MSELoss()\n",
    "        \n",
    "    def _sample_q(self, mu, log_var, use_means=False):\n",
    "        # sample z from parameterized distributions\n",
    "        std = torch.exp(log_var / 2)\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "        # get our latent\n",
    "        if use_means:\n",
    "            z = mu\n",
    "        else:\n",
    "            z = q.rsample()\n",
    "\n",
    "        return z, mu, std\n",
    "    \n",
    "    def _kl_divergence(self, z, mu, std):\n",
    "        # lightning imp.\n",
    "        # Monte carlo KL divergence\n",
    "        p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "\n",
    "        log_qzx = q.log_prob(z)\n",
    "        log_pz = p.log_prob(z)\n",
    "\n",
    "        kl = (log_qzx - log_pz)\n",
    "        kl = kl.sum(-1)\n",
    "\n",
    "        return kl\n",
    "    \n",
    "     def calculate_loss(self, x_raw, result):\n",
    "        nb = torch.distributions.NegativeBinomial(result['r'], result['p'])\n",
    "        reconstruction_loss = torch.mean(-nb.log_prob(x_raw))\n",
    "        \n",
    "        latent_loss = torch.mean(self.kl_divergence(result['z'], result['z_mu'], result['z_std']))\n",
    "        L_loss = torch.mean(self.kl_divergence(result['L'], result['L_mu'], result['L_std']))\n",
    "        E_loss = torch.mean(self.kl_divergence(result['E'], result['E_mu'], result['E_std']))\n",
    "        F_loss = torch.mean(self.kl_divergence(result['F'], result['F_mu'], result['F_std']))\n",
    "\n",
    "        kl_loss = (\n",
    "            latent_loss * self.latent_scaler,\n",
    "            L_loss * self.L_scaler,\n",
    "            E_loss * self.E_scaler,\n",
    "            F_loss * self.F_scaler\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'overall_loss': reconstruction_loss + self.kl_scaler * kl_loss,\n",
    "            'reconstruction_loss': reconstruction_loss,\n",
    "            'kl_loss': kl_loss,\n",
    "            'latent_loss': latent_loss,\n",
    "            'L_loss': L_loss,\n",
    "            'E_loss': E_loss,\n",
    "            'F_loss': F_loss,\n",
    "        }\n",
    "        \n",
    "    def encode(self, x, use_means=False):\n",
    "        x_encoded = self.unet(x)\n",
    "        \n",
    "        mu, log_var = self.latent_mu(x_encoded), self.latent_var(x_encoded)\n",
    "\n",
    "        return self._sample_q(mu, log_var, use_means=use_means), mu, log_var\n",
    "        \n",
    "    def sample_L(self, use_means=False):\n",
    "        mu, log_var = self.L_mu(self.L), self.L_var(self.L)\n",
    "\n",
    "        return self._sample_q(mu, log_var, use_means=use_means), mu, log_var\n",
    "    \n",
    "    def sample_E(self, use_means=False):\n",
    "        mu, log_var = self.E_mu(self.E), self.E_var(self.E)\n",
    "\n",
    "        return self._sample_q(mu, log_var, use_means=use_means), mu, log_var\n",
    "    \n",
    "    def sample_F(self, use_means=False):\n",
    "        mu, log_var = self.F_mu(self.F), self.F_var(self.F)\n",
    "\n",
    "        return self._sample_q(mu, log_var, use_means=use_means), mu, log_var\n",
    "        \n",
    "    def metagenes_from_latent(self, z):\n",
    "        h = self.h(z) # (b, m, H, W)\n",
    "        s = self.s(z) # (1, H, W, 1)\n",
    "        return rearrange(h * s, 'b c h w -> b h w c')\n",
    "    \n",
    "    def parameterize_nb(self, z, b, voxel_level=True, use_means=False, return_intermediates=False):\n",
    "        \"\"\"\n",
    "        reconstruct expression from latent z and covariate vector b\n",
    "        \n",
    "        z - (b, H, W, latent_dim)\n",
    "        b - (b, D)\n",
    "        \"\"\"\n",
    "        h = self.metagenes_from_latent(z) # (b, H, W, m)\n",
    "        \n",
    "        if voxel_level:\n",
    "            # aggregate and sum pixels to the voxel level where Hv and Wv are height and width in terms of voxels\n",
    "            h = reduce(h, 'b (h h2) (w w2) c -> b h w c', 'sum',\n",
    "                       h2=voxel_size, w2=voxel_size) # (b, Hv, Wv, m)\n",
    "        \n",
    "        # calculate r for each gene\n",
    "        L, L_mu, L_std = self.sample_L(use_means=use_means) # (m, g)\n",
    "        r = h@L # (b, Hv, Wv, g)\n",
    "        \n",
    "        # adjust for covariates\n",
    "        E, E_mu, E_std = self.sample_E(use_means=use_means) # (D, g)\n",
    "        adj = torch.exp(b@E + self.t) # (1, g)\n",
    "        r *= adj # (b, Hv, Wv, g)\n",
    "        \n",
    "        # calculate p\n",
    "        F, F_mu, F_std = self.sample_F(use_means=use_means)\n",
    "        p = torch.sigmoid(b@F + self.u) # (b, g)\n",
    "        \n",
    "        if return_intermediates:\n",
    "            return {\n",
    "                'L': L,\n",
    "                'L_mu': L_mu,\n",
    "                'L_std': L_std,\n",
    "                'E': E,\n",
    "                'E_mu': E_mu,\n",
    "                'E_std': E_std,\n",
    "                'F': F,\n",
    "                'F_mu': F_mu,\n",
    "                'F_std': F_std,\n",
    "                'metagene_matrix': h,\n",
    "                'r': r,\n",
    "                'p': p\n",
    "            }\n",
    "        \n",
    "        return r, p\n",
    "    \n",
    "    def forward_with_expression(self, x, b, use_means=False, voxel_level=True):\n",
    "        result = self.forward(x, b, voxel_level=voxel_level,\n",
    "                              use_means=use_means, return_intermediates=True)\n",
    "        r, p = result['r'], result['p']\n",
    "        \n",
    "        # get expression from negative binomial\n",
    "        nb = torch.distributions.NegativeBinomial(r, p)\n",
    "        result['expression'] = nb.mean()\n",
    "                \n",
    "        return result\n",
    "    \n",
    "    def forward(self, x, b, voxel_level=True, use_means=False, return_intermediates=True):\n",
    "        z, z_mu, z_std = self.encode(x, use_means=use_means)\n",
    "        result = self.parameterize_nb(z, b, voxel_level=voxel_level,\n",
    "                                      use_means=use_means, return_intermediates=return_intermediates)\n",
    "        result.update({\n",
    "            'z': z,\n",
    "            'z_mu': z_mu,\n",
    "            'z_std': z_std\n",
    "        })\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f02b5831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 5, 100])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = F.softplus(torch.randn(1, 5, 5, 100))\n",
    "p = torch.sigmoid(torch.randn(1, 100))\n",
    "\n",
    "nb = torch.distributions.NegativeBinomial(r, p)\n",
    "s = nb.sample()\n",
    "log_prob = -nb.log_prob(s)\n",
    "log_prob.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3a88878a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 10.0000, 180.0000,   1.0000])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c7c7ddde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 15., 208.,   2.]), tensor([3.2450, 5.0071, 2.0794]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = torch.Tensor([10., 20., 1.])\n",
    "p = torch.Tensor([.5, .9, .5])\n",
    "\n",
    "nb = torch.distributions.NegativeBinomial(r, p)\n",
    "s = nb.sample()\n",
    "log_prob = -nb.log_prob(s)\n",
    "s, log_prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2196f030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 5, 20])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(1, 50, 50, 20)\n",
    "reduce(a, 'b (h h2) (w w2) c -> b h w c', 'sum', h2=10, w2=10).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae9f9ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 5, 100])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(1, 5, 5, 20)\n",
    "b = torch.randn(20, 100)\n",
    "(a@b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "409beb5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 50, 20])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(1, 50, 50, 20)\n",
    "b = torch.randn(1, 20)\n",
    "(a * b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ac0319",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PollockModel(torch.nn.Module):\n",
    "    def __init__(self, genes, classes,\n",
    "                 latent_dim=64, enc_out_dim=128, middle_dim=512,\n",
    "                 zinb_scaler=1., kl_scaler=1e-5, clf_scaler=1.):\n",
    "        \"\"\"\n",
    "        Pollock VAE + classifier\n",
    "        \"\"\"\n",
    "        super(PollockModel, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.genes = genes\n",
    "        self.n_genes = len(genes)\n",
    "        self.classes = classes\n",
    "        self.n_classes = len(classes)\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.n_genes, middle_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(middle_dim, enc_out_dim),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.mu = torch.nn.Linear(enc_out_dim, latent_dim)\n",
    "        self.var = torch.nn.Linear(enc_out_dim, latent_dim)\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(latent_dim, enc_out_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(enc_out_dim, middle_dim),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.disp_decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(middle_dim, self.n_genes),\n",
    "            DispAct()\n",
    "        )\n",
    "        self.mean_decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(middle_dim, self.n_genes),\n",
    "            MeanAct()\n",
    "        )\n",
    "        self.drop_decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(middle_dim, self.n_genes),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.prediction_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(latent_dim, latent_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(latent_dim, self.n_classes),\n",
    "            torch.nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "        self.zinb_loss = ZINBLoss()\n",
    "        self.ce_loss = torch.nn.CrossEntropyLoss()\n",
    "        self.zinb_scaler = zinb_scaler\n",
    "        self.kl_scaler = kl_scaler\n",
    "        self.clf_scaler = clf_scaler\n",
    "\n",
    "    def kl_divergence(self, z, mu, std):\n",
    "        # lightning imp.\n",
    "        # Monte carlo KL divergence\n",
    "        p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "\n",
    "        log_qzx = q.log_prob(z)\n",
    "        log_pz = p.log_prob(z)\n",
    "\n",
    "        kl = (log_qzx - log_pz)\n",
    "        kl = kl.sum(-1)\n",
    "\n",
    "        return kl\n",
    "\n",
    "    def encode(self, x, use_means=False):\n",
    "        x_encoded = self.encoder(x)\n",
    "        mu, log_var = self.mu(x_encoded), self.var(x_encoded)\n",
    "\n",
    "        # sample z from parameterized distributions\n",
    "        std = torch.exp(log_var / 2)\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "        # get our latent\n",
    "        if use_means:\n",
    "            z = mu\n",
    "        else:\n",
    "            z = q.rsample()\n",
    "\n",
    "        return z, mu, std\n",
    "\n",
    "    def decode(self, x):\n",
    "        h = self.decoder(x)\n",
    "        x_disp = self.disp_decoder(h)\n",
    "        x_mean = self.mean_decoder(h)\n",
    "        x_drop = self.drop_decoder(h)\n",
    "\n",
    "        return x_disp, x_mean, x_drop\n",
    "\n",
    "    def calculate_loss(self, r, x_raw, scale_factor, y_true):\n",
    "        reconstruction_loss = self.zinb_loss(\n",
    "            x_raw, r['x_mean'], r['x_disp'], r['x_drop'], scale_factor=scale_factor)\n",
    "\n",
    "        kl_loss = torch.mean(self.kl_divergence(r['z'], r['mu'], r['std']))\n",
    "\n",
    "        clf_loss = torch.mean(self.ce_loss(r['y'], y_true))\n",
    "\n",
    "        return ((reconstruction_loss * self.zinb_scaler) + (kl_loss * self.kl_scaler) + (clf_loss * self.clf_scaler),\n",
    "                reconstruction_loss,\n",
    "                kl_loss,\n",
    "                clf_loss)\n",
    "\n",
    "    def forward(self, x, use_means=False):\n",
    "        z, mu, std = self.encode(x, use_means=use_means)\n",
    "        x_disp, x_mean, x_drop = self.decode(z)\n",
    "        y = self.prediction_head(z)\n",
    "\n",
    "        return {\n",
    "            'z': z,\n",
    "            'mu': mu,\n",
    "            'std': std,\n",
    "            'x_disp': x_disp,\n",
    "            'x_mean': x_mean,\n",
    "            'x_drop': x_drop,\n",
    "            'y': y\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0d8654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741f5450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dd9e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3ddb97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

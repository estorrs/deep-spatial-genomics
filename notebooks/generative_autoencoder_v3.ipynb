{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c8a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "from typing import Optional, List\n",
    "\n",
    "import scanpy as sc \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler, BatchSampler\n",
    "from torchvision.transforms import ColorJitter, Normalize, RandomHorizontalFlip, RandomVerticalFlip, RandomAdjustSharpness\n",
    "from timm import create_model\n",
    "from einops import rearrange, reduce\n",
    "from skimage.color import label2rgb\n",
    "from skimage.measure import regionprops_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e8c58b",
   "metadata": {},
   "source": [
    "\n",
    "note that you need to make bug fix to diffusers v0.3.0\n",
    "\n",
    "in ~/.local/lib/python3.9/site-packages/diffusers/models/unet_blocks.py you need to change out_channels parameter in DownEncoderBlock2D to make the unet work for >2 downsamples\n",
    "\n",
    "out_channels=in_channels if add_downsample else out_channels,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8aa08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210aae4a",
   "metadata": {},
   "source": [
    "#### load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a488573",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '../data/pytorch_datasets/pdac_v17/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173aecf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls ../data/pytorch_datasets/pdac_v8/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_voxels(padded_voxel_idxs):\n",
    "    idx = padded_voxel_idxs.flip((0,)).nonzero()[0].item()\n",
    "    return len(padded_voxel_idxs[:-idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6297aed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectorySTDataset(Dataset):\n",
    "    def __init__(self, directory, normalize=True):\n",
    "        super().__init__()\n",
    "        self.dir = directory\n",
    "        self.fps = [os.path.join(self.dir, fp) for fp in os.listdir(self.dir) if fp[-3:]=='.pt']\n",
    "        \n",
    "        if normalize:\n",
    "            self.normalize = Normalize((0.771, 0.651, 0.752), (0.229, 0.288, 0.224)) # from HT397B1-H2 ffpe H&E image\n",
    "        else:\n",
    "            self.normalize = nn.Identity()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.fps)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        fp = self.fps[idx]\n",
    "        obj = torch.load(fp)\n",
    "        img = TF.convert_image_dtype(obj['he'], dtype=torch.float32)\n",
    "\n",
    "        return {\n",
    "            'he': self.normalize(img),\n",
    "            'he_context': self.normalize(TF.convert_image_dtype(obj['he_context'], dtype=torch.float32)),\n",
    "            'he_orig': img,\n",
    "            'masks': obj['masks'],\n",
    "            'voxel_idxs': obj['voxel_idxs'],\n",
    "            'exp': obj['exp'],\n",
    "            'exp_tiles': obj['exp_tiles'],\n",
    "            'n_voxels': get_n_voxels(obj['voxel_idxs']),\n",
    "            'b': torch.tensor([1.])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62352413",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = DirectorySTDataset(os.path.join(dataset_dir, 'train', 'data'))\n",
    "# train_ds = DirectorySTDataset(os.path.join(dataset_dir, 'train'))\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991aed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# time.sleep(60 * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d053d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = DirectorySTDataset(os.path.join(dataset_dir, 'val', 'data'))\n",
    "# val_ds = DirectorySTDataset(os.path.join(dataset_dir, 'val'))\n",
    "len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57deaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.path.join(dataset_dir, 'train', 'adatas')\n",
    "# directory = os.path.join(dataset_dir, 'train')\n",
    "fps = [os.path.join(directory, fp) for fp in os.listdir(directory) if fp[-5:]=='.h5ad']\n",
    "sample_to_train_adata = {}\n",
    "for fp in fps:\n",
    "    s = fp.split('/')[-1].replace('.h5ad', '')\n",
    "    sample_to_train_adata[s] = sc.read_h5ad(fp)\n",
    "sample_to_train_adata.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a49cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.path.join(dataset_dir, 'val', 'adatas')\n",
    "# directory = os.path.join(dataset_dir, 'val')\n",
    "fps = [os.path.join(directory, fp) for fp in os.listdir(directory) if fp[-5:]=='.h5ad']\n",
    "sample_to_val_adata = {}\n",
    "for fp in fps:\n",
    "    s = fp.split('/')[-1].replace('.h5ad', '')\n",
    "    sample_to_val_adata[s] = sc.read_h5ad(fp)\n",
    "sample_to_val_adata.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c827ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=1)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b7bc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cec3f30",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787dfb6f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cte(padded_exp, masks, n_voxels):\n",
    "    tile = torch.zeros((masks.shape[1], masks.shape[2], padded_exp.shape[1]))\n",
    "    for exp, m in list(zip(padded_exp, masks))[:n_voxels]:\n",
    "        tile[m==1] = exp.to(torch.float32)\n",
    "    return tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbdf8ce",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    print(i)\n",
    "    img = rearrange(train_ds[i]['he'], 'c h w -> h w c')\n",
    "    img -= img.min()\n",
    "    img /= img.max()\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8646c1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# i = 166\n",
    "# i = 66\n",
    "# i = 22\n",
    "i = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb596e12",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img = rearrange(train_ds[i]['he'], 'c h w -> h w c')\n",
    "img -= img.min()\n",
    "img /= img.max()\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc79b4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img = rearrange(train_ds[i]['he_orig'], 'c h w -> h w c')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7271895c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(torch.sum(train_ds[i]['masks'], dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20504c5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for j in range(train_ds[i]['n_voxels']):\n",
    "    plt.imshow(train_ds[i]['masks'][j])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6d20d0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.sum(train_ds[i]['masks'], dim=(-1, -2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3798644",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gene = 'IL7R'\n",
    "recon = cte(train_ds[i]['exp'], train_ds[i]['masks'], train_ds[i]['n_voxels'])\n",
    "plt.imshow(recon[:, :, train_adata.var.index.to_list().index(gene)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a2b55b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "recon[:, :, train_adata.var.index.to_list().index(gene)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a84efc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(train_ds[i]['exp_tiles'][:, :, train_adata.var.index.to_list().index(gene)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ead9ec8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_ds[i]['exp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a56200",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_ds[i]['voxel_idxs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc378578",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pool = set(train_ds[i]['voxel_idxs'][:train_ds[i]['n_voxels']].detach().numpy())\n",
    "train_adata.obs['highlight'] = ['yes' if i in pool else 'no'\n",
    "                               for i in train_adata.obs['spot_index']]\n",
    "sc.pl.spatial(train_adata, color='highlight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b397e3e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sc.pl.spatial(train_adata, color='IL7R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360b5ce8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2f4232",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c7c0c4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56339c40",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img = rearrange(val_ds[i]['he_orig'], 'c h w -> h w c')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b40db0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gene = 'EPCAM'\n",
    "recon = cte(val_ds[i]['exp'], val_ds[i]['masks'], val_ds[i]['n_voxels'])\n",
    "plt.imshow(recon[:, :, val_adata.var.index.to_list().index(gene)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa221d2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pool = set(val_ds[i]['voxel_idxs'][:val_ds[i]['n_voxels']].detach().numpy())\n",
    "val_adata.obs['highlight'] = ['yes' if i in pool else 'no'\n",
    "                               for i in val_adata.obs['spot_index']]\n",
    "sc.pl.spatial(val_adata, color='highlight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cce34b5",
   "metadata": {},
   "source": [
    "#### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621d11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "modified from https://gist.github.com/rwightman/f8b24f4e6f5504aba03e999e02460d31\n",
    "\"\"\"\n",
    "class Unet(nn.Module):\n",
    "    \"\"\"Unet is a fully convolution neural network for image semantic segmentation\n",
    "    Args:\n",
    "        encoder_name: name of classification model (without last dense layers) used as feature\n",
    "            extractor to build segmentation model.\n",
    "        encoder_weights: one of ``None`` (random initialization), ``imagenet`` (pre-training on ImageNet).\n",
    "        decoder_channels: list of numbers of ``Conv2D`` layer filters in decoder blocks\n",
    "        decoder_use_batchnorm: if ``True``, ``BatchNormalisation`` layer between ``Conv2D`` and ``Activation`` layers\n",
    "            is used.\n",
    "        num_classes: a number of classes for output (output shape - ``(batch, classes, h, w)``).\n",
    "        center: if ``True`` add ``Conv2dReLU`` block on encoder head\n",
    "    NOTE: This is based off an old version of Unet in https://github.com/qubvel/segmentation_models.pytorch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            backbone='resnet34',\n",
    "            backbone_kwargs=None,\n",
    "            backbone_indices=None,\n",
    "            decoder_use_batchnorm=True,\n",
    "            decoder_channels=(256, 128, 64, 32, 16),\n",
    "            in_chans=1,\n",
    "            num_classes=5,\n",
    "            center=False,\n",
    "            norm_layer=nn.BatchNorm2d,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        backbone_kwargs = backbone_kwargs or {}\n",
    "        # NOTE some models need different backbone indices specified based on the alignment of features\n",
    "        # and some models won't have a full enough range of feature strides to work properly.\n",
    "        encoder = create_model(\n",
    "            backbone, features_only=True, out_indices=backbone_indices, in_chans=in_chans,\n",
    "            pretrained=False, **backbone_kwargs)\n",
    "        encoder_channels = encoder.feature_info.channels()[::-1]\n",
    "        self.encoder = encoder\n",
    "\n",
    "        if not decoder_use_batchnorm:\n",
    "            norm_layer = None\n",
    "        self.decoder = UnetDecoder(\n",
    "            encoder_channels=encoder_channels,\n",
    "            decoder_channels=decoder_channels,\n",
    "            final_channels=num_classes,\n",
    "            norm_layer=norm_layer,\n",
    "            center=center,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.encoder(x)\n",
    "        x.reverse()  # torchscript doesn't work with [::-1]\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conv2dBnAct(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=0,\n",
    "                 stride=1, act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = norm_layer(out_channels)\n",
    "        self.act = act_layer(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, scale_factor=2.0, act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d):\n",
    "        super().__init__()\n",
    "        conv_args = dict(kernel_size=3, padding=1, act_layer=act_layer)\n",
    "        self.scale_factor = scale_factor\n",
    "        if norm_layer is None:\n",
    "            self.conv1 = Conv2dBnAct(in_channels, out_channels, **conv_args)\n",
    "            self.conv2 = Conv2dBnAct(out_channels, out_channels,  **conv_args)\n",
    "        else:\n",
    "            self.conv1 = Conv2dBnAct(in_channels, out_channels, norm_layer=norm_layer, **conv_args)\n",
    "            self.conv2 = Conv2dBnAct(out_channels, out_channels, norm_layer=norm_layer, **conv_args)\n",
    "\n",
    "    def forward(self, x, skip: Optional[torch.Tensor] = None):\n",
    "        if self.scale_factor != 1.0:\n",
    "            x = F.interpolate(x, scale_factor=self.scale_factor, mode='nearest')\n",
    "        if skip is not None:\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UnetDecoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            encoder_channels,\n",
    "            decoder_channels=(256, 128, 64, 32, 16),\n",
    "            final_channels=1,\n",
    "            norm_layer=nn.BatchNorm2d,\n",
    "            center=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if center:\n",
    "            channels = encoder_channels[0]\n",
    "            self.center = DecoderBlock(channels, channels, scale_factor=1.0, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.center = nn.Identity()\n",
    "\n",
    "        in_channels = [in_chs + skip_chs for in_chs, skip_chs in zip(\n",
    "            [encoder_channels[0]] + list(decoder_channels[:-1]),\n",
    "            list(encoder_channels[1:]) + [0])]\n",
    "        out_channels = decoder_channels\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for in_chs, out_chs in zip(in_channels, out_channels):\n",
    "            self.blocks.append(DecoderBlock(in_chs, out_chs, norm_layer=norm_layer))\n",
    "        self.final_conv = nn.Conv2d(out_channels[-1], final_channels, kernel_size=(1, 1))\n",
    "\n",
    "        self._init_weight()\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x: List[torch.Tensor]):\n",
    "        encoder_head = x[0]\n",
    "        skips = x[1:]\n",
    "        x = self.center(encoder_head)\n",
    "        for i, b in enumerate(self.blocks):\n",
    "            skip = skips[i] if i < len(skips) else None\n",
    "            x = b(x, skip)\n",
    "        x = self.final_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d299f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetBased(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        genes,\n",
    "        tile_resolution = 16,\n",
    "        n_metagenes = 20,\n",
    "        in_channels = 3,\n",
    "        out_channels = 64,\n",
    "        decoder_channels = (128, 64, 32, 16, 8),\n",
    "        context_decoder_channels = (128, 64, 32, 16, 8),\n",
    "        he_scaler = .1,\n",
    "        kl_scaler = .001,\n",
    "        exp_scaler = 1.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.genes = genes\n",
    "        self.n_genes = len(genes)\n",
    "\n",
    "        \n",
    "        self.he_scaler = he_scaler\n",
    "        self.kl_scaler = kl_scaler\n",
    "        self.exp_scaler = exp_scaler\n",
    "        \n",
    "        self.unet = Unet(backbone='resnet34',\n",
    "                         decoder_channels=decoder_channels,\n",
    "                         in_chans=in_channels,\n",
    "                         num_classes=out_channels)\n",
    "        \n",
    "        self.context_unet = Unet(backbone='resnet34',\n",
    "                         decoder_channels=context_decoder_channels,\n",
    "                         in_chans=in_channels,\n",
    "                         num_classes=out_channels)\n",
    "        \n",
    "        self.post_unet_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=out_channels * 2, out_channels=out_channels, kernel_size=1),\n",
    "            nn.AvgPool2d(kernel_size=tile_resolution)\n",
    "        )\n",
    "\n",
    "        # latent mu and var\n",
    "        self.latent_mu = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=1)\n",
    "        self.latent_var = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=1)\n",
    "        \n",
    "        self.n_metagenes = n_metagenes\n",
    "        self.tile_resolution = tile_resolution\n",
    "        self.metagenes = torch.nn.Parameter(torch.rand(self.n_metagenes, self.n_genes))\n",
    "        self.scale_factors = torch.nn.Parameter(torch.rand(self.n_genes))\n",
    "        self.p = torch.nn.Parameter(torch.rand(self.n_genes))\n",
    "        \n",
    "        self.post_decode_he = torch.nn.Conv2d(out_channels, 3, 1)\n",
    "        self.post_decode_exp = torch.nn.Conv2d(out_channels, self.n_metagenes, 1)\n",
    "        \n",
    "        self.he_loss = torch.nn.MSELoss()\n",
    "        \n",
    "    def _kl_divergence(self, z, mu, std):\n",
    "        # lightning imp.\n",
    "        # Monte carlo KL divergence\n",
    "        p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "\n",
    "        log_qzx = q.log_prob(z)\n",
    "        log_pz = p.log_prob(z)\n",
    "\n",
    "        kl = (log_qzx - log_pz)\n",
    "        kl = kl.sum(-1)\n",
    "\n",
    "        return kl\n",
    "\n",
    "    def encode(self, x, x_context, use_means=False):\n",
    "        x_encoded = self.unet(x)\n",
    "        x_context_encoded = self.context_unet(x)\n",
    "        \n",
    "        x_encoded = torch.concat((x_encoded, x_context_encoded), dim=1)\n",
    "        x_encoded = self.post_unet_conv(x_encoded)\n",
    "        \n",
    "        mu, log_var = self.latent_mu(x_encoded), self.latent_var(x_encoded)\n",
    "        \n",
    "        # sample z from parameterized distributions\n",
    "        std = torch.exp(log_var / 2)\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "        # get our latent\n",
    "        if use_means:\n",
    "            z = mu\n",
    "        else:\n",
    "            z = q.rsample()\n",
    "\n",
    "        return z, mu, std\n",
    "    \n",
    "    def calculate_loss(self, exp_true, result):\n",
    "        exp_loss = torch.mean(-result['nb'].log_prob(exp_true))\n",
    "        \n",
    "        kl_loss = torch.mean(self._kl_divergence(result['z'], result['z_mu'], result['z_std']))\n",
    "        \n",
    "#         he_loss = torch.mean(self.he_loss(he_true, result['he']))\n",
    "        \n",
    "        return {\n",
    "            'overall_loss': exp_loss * self.exp_scaler + kl_loss * self.kl_scaler,\n",
    "            'exp_loss': exp_loss,\n",
    "            'kl_loss': kl_loss,\n",
    "#             'he_loss': he_loss\n",
    "        }\n",
    "    \n",
    "    def reconstruct_expression(self, dec, reduce_to_tile=True):\n",
    "        x = self.post_decode_exp(dec)\n",
    "        \n",
    "#         if reduce_to_tile:\n",
    "#             x = reduce(x, 'b c (h1 h2) (w1 w2) -> b c h1 w1', 'sum', h2=self.tile_resolution, w2=self.tile_resolution)\n",
    "            \n",
    "        x = rearrange(x, 'b c h w -> b h w c')\n",
    "        \n",
    "        r = x @ self.metagenes\n",
    "        r = r * self.scale_factors\n",
    "        r = F.softplus(r)\n",
    "        r += .00000001\n",
    "        \n",
    "        p = torch.sigmoid(self.p)\n",
    "        p = rearrange(p, 'c -> 1 1 1 c')\n",
    "        \n",
    "        nb = torch.distributions.NegativeBinomial(r, p)\n",
    "        \n",
    "        return {\n",
    "            'r': r,\n",
    "            'p': p,\n",
    "            'exp': nb.mean,\n",
    "            'nb': nb,\n",
    "            'metagene_activity': x # (b, h w m)\n",
    "        }\n",
    "    \n",
    "#     def reconstruct_he(self, dec):\n",
    "#         he = self.post_decode_he(dec)\n",
    "#         return he\n",
    "\n",
    "    def forward(self, x, x_context, reduce_to_tile=True, use_means=False):\n",
    "        z, z_mu, z_std = self.encode(x, x_context, use_means=use_means)\n",
    "\n",
    "#         he = self.reconstruct_he(z)\n",
    "        \n",
    "        exp_result = self.reconstruct_expression(z, reduce_to_tile=reduce_to_tile)\n",
    "        \n",
    "        result = {\n",
    "            'z': z,\n",
    "            'z_mu': z_mu,\n",
    "            'z_std': z_std,\n",
    "#             'he': he,\n",
    "        }\n",
    "        result.update(exp_result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc53b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_intermediates(logger, batch, result, plot_genes, model,\n",
    "                      n_samples=8, result_full_res=None, identifier='train'):\n",
    "    model_genes = np.asarray(model.genes)\n",
    "    g2i = {g:i for i, g in enumerate(model_genes)}\n",
    "    gene_idxs = np.asarray([g2i[g] for g in plot_genes])\n",
    "    \n",
    "    img = batch['he_context'][:n_samples].clone().detach()\n",
    "    img -= img.min()\n",
    "    img /= img.max()\n",
    "    logger.log_image(\n",
    "        key=f\"{identifier}/he_context\",\n",
    "        images=[img],\n",
    "        caption=[f'{identifier} he context']\n",
    "    )\n",
    "    \n",
    "    img = batch['he'][:n_samples].clone().detach()\n",
    "    img -= img.min()\n",
    "    img /= img.max()\n",
    "    logger.log_image(\n",
    "        key=f\"{identifier}/he_groundtruth\",\n",
    "        images=[img],\n",
    "        caption=[f'{identifier} he tile 2x']\n",
    "    )\n",
    "    \n",
    "#     img = result['he'][:n_samples].clone().detach() # (b c h w)\n",
    "#     img -= img.min()\n",
    "#     img /= img.max()\n",
    "#     logger.log_image(\n",
    "#         key=f\"{identifier}/he_reconstruction\",\n",
    "#         images=[img],\n",
    "#         caption=[f'{identifier} he tile recon']\n",
    "#     )\n",
    "\n",
    "    recon = batch['exp_tiles'][:n_samples].clone().detach().to(torch.float16)\n",
    "    recon = recon[:, :, :, gene_idxs]\n",
    "    recon -= recon.min()\n",
    "    recon /= recon.max()\n",
    "    recon = rearrange(recon, 'b h w c -> c b 1 h w')\n",
    "    logger.log_image(\n",
    "        key=f\"{identifier}/exp_groundtruth\",\n",
    "        images=[img for img in recon],\n",
    "        caption=[g for g in plot_genes]\n",
    "    )\n",
    "\n",
    "    recon = result['exp'][:n_samples].clone().detach().to(torch.float16)\n",
    "    recon = recon[:, :, :, gene_idxs]\n",
    "    recon -= recon.min()\n",
    "    recon /= recon.max()\n",
    "    recon = rearrange(recon, 'b h w c -> c b 1 h w')\n",
    "    logger.log_image(\n",
    "        key=f\"{identifier}/exp_reconstruction\",\n",
    "        images=[img for img in recon],\n",
    "        caption=[g for g in plot_genes]\n",
    "    )\n",
    "    \n",
    "    recon = result['metagene_activity'][:1].clone().detach().to(torch.float32)\n",
    "    recon -= recon.min()\n",
    "    recon /= recon.max()\n",
    "    recon = rearrange(recon, 'b h w c -> c b 1 h w')\n",
    "    logger.log_image(\n",
    "        key=f\"{identifier}/metagene_activity\",\n",
    "        images=[img for img in recon],\n",
    "        caption=list(range(model.n_metagenes))\n",
    "    )\n",
    "    \n",
    "    vals = model.metagenes.clone().detach().cpu().numpy()\n",
    "    vals = vals[:, gene_idxs]\n",
    "    df = pd.DataFrame(data=vals, columns=plot_genes)\n",
    "    logger.log_text(\n",
    "        key=f'{identifier}/scale_factors',\n",
    "        dataframe=df\n",
    "    )\n",
    "    \n",
    "    vals = model.scale_factors.clone().detach().cpu().numpy()\n",
    "    vals = vals[gene_idxs]\n",
    "    df = pd.DataFrame(data=[vals], columns=plot_genes)\n",
    "    logger.log_text(\n",
    "        key=f'{identifier}/scale_factors',\n",
    "        dataframe=df\n",
    "    )\n",
    "    \n",
    "    if result_full_res is not None:\n",
    "        recon = result_full_res['exp'][:n_samples].clone().detach().to(torch.float16)\n",
    "        recon = recon[:, :, :, gene_idxs]\n",
    "        recon = rearrange(recon, 'b h w c -> c b 1 h w')\n",
    "        logger.log_image(\n",
    "            key=f\"{identifier}/full_res_exp_reconstruction\",\n",
    "            images=[img for img in recon],\n",
    "            caption=[g for g in plot_genes]\n",
    "        )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e71eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class xFuseLightning(pl.LightningModule):\n",
    "    def __init__(self, autokl, lr=1e-3, n_samples=16, plot_genes=['IL7R', 'KRT18', 'BGN', 'PECAM1'],\n",
    "                 train_epoch_fraction=.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.autokl = autokl\n",
    "        self.lr = lr\n",
    "        self.plot_genes = plot_genes\n",
    "        self.n_samples = n_samples\n",
    "        self.train_epoch_fraction = train_epoch_fraction\n",
    "        \n",
    "        self.save_hyperparameters(ignore=['autokl'])\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, x_context, b, masks, voxel_idxs, exp, exp_tiles = batch['he'], batch['he_context'], batch['b'], batch['masks'], batch['voxel_idxs'], batch['exp'], batch['exp_tiles']\n",
    "        result = self.autokl(x, x_context)\n",
    "        losses = self.autokl.calculate_loss(exp_tiles, result)\n",
    "        losses = {f'train/{k}':v for k, v in losses.items()}\n",
    "        self.log_dict(losses, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        losses['loss'] = losses['train/overall_loss']\n",
    "        \n",
    "        # only log 10-ish% of training epochs\n",
    "        if batch_idx == 0 and torch.rand(1).item() < self.train_epoch_fraction:\n",
    "            result_full_res = self.autokl(x[:1], x_context[:1], reduce_to_tile=False)\n",
    "            log_intermediates(self.logger, batch, result, self.plot_genes, self.autokl,\n",
    "                              n_samples=self.n_samples, result_full_res=result_full_res, identifier='train')\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, x_context, b, masks, voxel_idxs, exp, exp_tiles = batch['he'], batch['he_context'], batch['b'], batch['masks'], batch['voxel_idxs'], batch['exp'], batch['exp_tiles']\n",
    "        result = self.autokl(x, x_context)\n",
    "        losses = self.autokl.calculate_loss(exp_tiles, result)\n",
    "        losses = {f'val/{k}':v for k, v in losses.items()}\n",
    "        self.log_dict(losses, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            result_full_res = self.autokl(x[:1], x_context[:1], reduce_to_tile=False)\n",
    "            log_intermediates(self.logger, batch, result, self.plot_genes, self.autokl,\n",
    "                              n_samples=self.n_samples, result_full_res=result_full_res, identifier='val')\n",
    "        \n",
    "        return losses\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f085f0d9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### test forwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc6928",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_dl))\n",
    "x, b, masks, voxel_idxs, exp, exp_tiles = batch['he'], batch['b'], batch['masks'], batch['voxel_idxs'], batch['exp'], batch['exp_tiles']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e1360b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# autokl = AutoencoderKL(\n",
    "#     train_adata.shape[1],\n",
    "#     tile_resolution=32,\n",
    "#     n_metagenes=20,\n",
    "#     in_channels=3,\n",
    "#     out_channels=64,\n",
    "#     down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\", \"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n",
    "#     up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\", \"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n",
    "#     block_out_channels=[8, 16, 32, 64],\n",
    "#     norm_num_groups=8,\n",
    "#     latent_channels=4,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87370a52",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "autokl = UnetBased(\n",
    "    train_adata.var.index.to_list(),\n",
    "    tile_resolution=32,\n",
    "    n_metagenes=20,\n",
    "    in_channels=3,\n",
    "    out_channels=64,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e145a917",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result = autokl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e481b0b8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5339d5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51edb14e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1716e7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result['exp'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18204c7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(result['exp'][0, :, :, 5].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8922998",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(result['metagene_activity'][0, :, :, 0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0802b2e9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "losses = autokl.calculate_loss(x, exp_tiles, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da29d920",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412f6363",
   "metadata": {},
   "source": [
    "#### training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b860e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'unet_based'\n",
    "log_dir = '/scratch1/fs1/dinglab/estorrs/deep-spatial-genomics/logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0237ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "logger = WandbLogger(project=project, save_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8d21d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805e75b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'n_genes': next(iter(sample_to_train_adata.values())).shape[1],\n",
    "    'genes': next(iter(sample_to_train_adata.values())).var.index.to_numpy(),\n",
    "    'n_covariates': 1,\n",
    "    'n_metagenes': 10,\n",
    "    'latent_dim': 64,\n",
    "    'tile_resolution': 32,\n",
    "    'he_scale': '2X',\n",
    "    'he_context_scale': '8X',\n",
    "    'encoder': {\n",
    "        'model': 'unet',\n",
    "        'in_channels': 3,\n",
    "        'decoder_channels': (128, 64, 32, 16, 8),\n",
    "        'context_decoder_channels': (128, 64, 32, 16, 8),\n",
    "    },\n",
    "    'kl_scalers': {\n",
    "        'exp_scaler': 1.,\n",
    "        'kl_scaler': .0001,\n",
    "        'he_scaler': .1,\n",
    "    },\n",
    "    'training': {\n",
    "        'train_samples': list(sample_to_train_adata.keys()),\n",
    "        'val_samples': list(sample_to_val_adata.keys()),\n",
    "        'log_n_samples': 8,\n",
    "        'max_epochs': 200,\n",
    "        'check_val_every_n_epoch': 10,\n",
    "        'log_train_fraction': 1.,\n",
    "        'log_every_n_steps': 1,\n",
    "        'accelerator': 'gpu',\n",
    "        'devices': 1,\n",
    "        'limit_train_batches': 1.,\n",
    "        'limit_val_batches': .1,\n",
    "        'lr': 1e-5,\n",
    "        'batch_size': batch_size,\n",
    "        'precision': 32\n",
    "    },\n",
    "}\n",
    "logger.experiment.config.update(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f8cca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "autokl = UnetBased(\n",
    "    config['genes'],\n",
    "    tile_resolution=config['tile_resolution'],\n",
    "    n_metagenes=config['n_metagenes'],\n",
    "    in_channels=config['encoder']['in_channels'],\n",
    "    out_channels=config['latent_dim'],\n",
    "    decoder_channels=config['encoder']['decoder_channels'],\n",
    "    context_decoder_channels=config['encoder']['context_decoder_channels'],\n",
    "    he_scaler=config['kl_scalers']['he_scaler'],\n",
    "    kl_scaler=config['kl_scalers']['kl_scaler'],\n",
    "    exp_scaler=config['kl_scalers']['exp_scaler'],\n",
    ")\n",
    "model = xFuseLightning(autokl, lr=config['training']['lr'],\n",
    "                       n_samples=config['training']['log_n_samples'],\n",
    "                       train_epoch_fraction=config['training']['log_train_fraction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816cdca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    devices=config['training']['devices'],\n",
    "    accelerator=config['training']['accelerator'],\n",
    "    check_val_every_n_epoch=config['training']['check_val_every_n_epoch'],\n",
    "    enable_checkpointing=False,\n",
    "    limit_val_batches=config['training']['limit_val_batches'],\n",
    "    limit_train_batches=config['training']['limit_train_batches'],\n",
    "    log_every_n_steps=config['training']['log_every_n_steps'],\n",
    "    max_epochs=config['training']['max_epochs'],\n",
    "    precision=config['training']['precision'],\n",
    "    logger=logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430a05b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.fit(model=model, train_dataloaders=train_dl, val_dataloaders=val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d95f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/scratch1/fs1/dinglab/estorrs/deep-spatial-genomics/runs/xfuse_improved_v4/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17476978",
   "metadata": {},
   "outputs": [],
   "source": [
    "autokl = UnetBased(\n",
    "    config['genes'],\n",
    "    tile_resolution=config['tile_resolution'],\n",
    "    n_metagenes=config['n_metagenes'],\n",
    "    in_channels=config['encoder']['in_channels'],\n",
    "    out_channels=config['latent_dim'],\n",
    "    decoder_channels=config['encoder']['decoder_channels'],\n",
    "    context_decoder_channels=config['encoder']['context_decoder_channels'],\n",
    "    he_scaler=config['kl_scalers']['he_scaler'],\n",
    "    kl_scaler=config['kl_scalers']['kl_scaler'],\n",
    "    exp_scaler=config['kl_scalers']['exp_scaler'],\n",
    ")\n",
    "model = xFuseLightning(autokl, lr=config['training']['lr'],\n",
    "                       n_samples=config['training']['log_n_samples'],\n",
    "                       train_epoch_fraction=config['training']['log_train_fraction'])\n",
    "model.load_state_dict(torch.load('/scratch1/fs1/dinglab/estorrs/deep-spatial-genomics/runs/xfuse_improved_v4/model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb6e7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ced218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = next(iter(sample_to_train_adata.values()))\n",
    "# a = sample_to_val_adata['HT264P1-S1H2U1']\n",
    "a = sample_to_train_adata['HT270P1-S1H1U1']\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc71724",
   "metadata": {},
   "outputs": [],
   "source": [
    "he = a.uns['rescaled_he']['2X_notrim']\n",
    "# he = a.uns['rescaled_he']['2X_trimmed']\n",
    "\n",
    "he.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc485c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(he)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25194ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_he = a.uns['rescaled_he']['8X_notrim']\n",
    "# context_he = a.uns['rescaled_he']['8X_trimmed']\n",
    "context_he.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b1fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(context_he)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284e98ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "he.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c01d4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflection_mosiac(x, border=256):\n",
    "    max_r, max_c = x.shape[-2], x.shape[-1]\n",
    "    if len(x.shape) == 3:\n",
    "        mosaic = torch.zeros((x.shape[0], max_r + (border * 2), max_c + (border * 2))).to(x.dtype)\n",
    "    else:\n",
    "        mosaic = torch.zeros((max_r + (border * 2), max_c + (border * 2))).to(x.dtype)\n",
    "    \n",
    "    # make tiles\n",
    "    top_left = TF.pad(x, padding=[border, border, 0, 0], padding_mode='reflect')\n",
    "    top_right = TF.pad(x, padding=[0, border, border, 0], padding_mode='reflect')\n",
    "    bottom_left = TF.pad(x, padding=[border, 0, 0, border], padding_mode='reflect')\n",
    "    bottom_right = TF.pad(x, padding=[0, 0, border, border], padding_mode='reflect')\n",
    "    \n",
    "    if len(x.shape) == 3:\n",
    "        mosaic[:, :max_r + border, :max_c + border] = top_left\n",
    "        mosaic[:, :max_r + border, border:] = top_right\n",
    "        mosaic[:, border:, :max_c + border] = bottom_left\n",
    "        mosaic[:, border:, border:] = bottom_right\n",
    "    else:\n",
    "        mosaic[:max_r + border, :max_c + border] = top_left\n",
    "        mosaic[:max_r + border, border:] = top_right\n",
    "        mosaic[border:, :max_c + border] = bottom_left\n",
    "        mosaic[border:, border:] = bottom_right\n",
    "    \n",
    "    return mosaic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58545c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_img(img, context_img, context_scale, tile_size=256, window_scale=2):\n",
    "    context_img = reflection_mosiac(context_img, border=tile_size)\n",
    "    \n",
    "    n_rows = (img.shape[1] // tile_size) * window_scale + 1\n",
    "    n_cols = (img.shape[2] // tile_size) * window_scale + 1\n",
    "    \n",
    "    tiles = torch.ones(n_rows * n_cols, img.shape[0], tile_size, tile_size, dtype=img.dtype)\n",
    "    expanded = torch.ones(img.shape[0],\n",
    "                          n_rows * tile_size // window_scale + tile_size,\n",
    "                          n_cols * tile_size // window_scale + tile_size,\n",
    "                          dtype=img.dtype)\n",
    "    expanded[:, :img.shape[1], :img.shape[2]] = img\n",
    "    \n",
    "    context_tiles = torch.ones(n_rows * n_cols, img.shape[0], tile_size, tile_size, dtype=context_img.dtype)\n",
    "    \n",
    "    idx = 0\n",
    "    top_left = []\n",
    "    for r in range(n_rows):\n",
    "        for c in range(n_cols):\n",
    "            r1 = r * tile_size // window_scale\n",
    "            c1 = c * tile_size // window_scale\n",
    "            r2 = r1 + tile_size\n",
    "            c2 = c1 + tile_size\n",
    "            tiles[idx] = expanded[:, r1:r2, c1:c2]\n",
    "            top_left.append((r, c))\n",
    "            \n",
    "            center_r = r1 + (tile_size / 2)\n",
    "            center_c = c1 + (tile_size / 2)\n",
    "            center_r = int(center_r / context_scale) \n",
    "            center_c = int(center_c / context_scale) \n",
    "            r1 = center_r - tile_size // 2 + tile_size\n",
    "            c1 = center_c - tile_size // 2 + tile_size\n",
    "            r2 = r1 + tile_size\n",
    "            c2 = c1 + tile_size\n",
    "            context_tiles[idx] = context_img[:, r1:r2, c1:c2]\n",
    "            idx += 1\n",
    "    return tiles, context_tiles, top_left       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5411506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_img(img):\n",
    "    if img.shape[0] != 3:\n",
    "        img = rearrange(img, 'h w c -> c h w')\n",
    "    if not isinstance(img, torch.Tensor):\n",
    "        img = torch.tensor(img)\n",
    "    if not img.dtype == torch.float32:\n",
    "        img = TF.convert_image_dtype(img, dtype=torch.float32)\n",
    "    \n",
    "    return img \n",
    "\n",
    "class TiledHEDataset(Dataset):\n",
    "    def __init__(self, img, context_img, context_scale, normalize=True, tile_size=256, window_scale=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        img = prepare_img(img)\n",
    "        context_img = prepare_img(context_img)\n",
    "\n",
    "        self.img = img\n",
    "        self.context_img = context_img\n",
    "        self.context_scale = context_scale\n",
    "        self.tiles, self.context_tiles, self.idx_to_top_left = tile_img(img, context_img, context_scale,\n",
    "                                                                        tile_size=tile_size, window_scale=window_scale)\n",
    "        \n",
    "        if normalize:\n",
    "            self.normalize = Normalize((0.771, 0.651, 0.752), (0.229, 0.288, 0.224)) # from HT397B1-H2 ffpe H&E image\n",
    "        else:\n",
    "            self.normalize = nn.Identity()\n",
    "            \n",
    "    def max_r(self):\n",
    "        rs, cs = zip(*self.idx_to_top_left)\n",
    "        return np.max(rs)\n",
    "\n",
    "    def max_c(self):\n",
    "        rs, cs = zip(*self.idx_to_top_left)\n",
    "        return np.max(cs)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.tiles.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tile, context_tile, (r, c) = self.tiles[idx], self.context_tiles[idx], self.idx_to_top_left[idx]\n",
    "#         print(tile.shape, context_tile.shape, r, c)\n",
    "        return {\n",
    "            'he': self.normalize(tile),\n",
    "            'context_he': self.normalize(context_tile),\n",
    "            'top': r,\n",
    "            'left': c\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f78e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tile(tile, new, r, c, max_r, max_c, window_scale=2,):\n",
    "    ts = tile.shape[0] // window_scale\n",
    "    normal_offset = ts // 2\n",
    "    edge_offset = tile.shape[0] // 2\n",
    "    rc, cc = r * ts + edge_offset, c * ts + edge_offset\n",
    "    trc, tcc = edge_offset, edge_offset\n",
    "\n",
    "    r1 = rc - normal_offset\n",
    "    r2 = rc + normal_offset\n",
    "    c1 = cc - normal_offset\n",
    "    c2 = cc + normal_offset\n",
    "    tr1 = trc - normal_offset\n",
    "    tr2 = trc + normal_offset\n",
    "    tc1 = tcc - normal_offset\n",
    "    tc2 = tcc + normal_offset\n",
    "    if r == 0:\n",
    "        r1 = rc - edge_offset\n",
    "        r2 = rc + normal_offset\n",
    "        tr1 = trc - edge_offset\n",
    "        tr2 = trc + normal_offset\n",
    "    if c == 0:\n",
    "        c1 = cc - edge_offset\n",
    "        c2 = cc + normal_offset\n",
    "        tc1 = tcc - edge_offset\n",
    "        tc2 = tcc + normal_offset\n",
    "    if r == max_r:\n",
    "        r1 = rc - normal_offset\n",
    "        r2 = rc + edge_offset\n",
    "        tr1 = trc - normal_offset\n",
    "        tr2 = trc + edge_offset\n",
    "    if c == max_c:\n",
    "        c1 = cc - normal_offset\n",
    "        c2 = cc + edge_offset\n",
    "        tc1 = tcc - normal_offset\n",
    "        tc2 = tcc + edge_offset\n",
    "\n",
    "    new[r1:r2, c1:c2] = tile[tr1:tr2, tc1:tc2]\n",
    "\n",
    "def predict_he(img, context_img, context_scale, model,\n",
    "               batch_size=8, reduce_to_tile=True,\n",
    "               gene_idxs=None, keep=('he', 'exp', 'metagene_activity'),\n",
    "               window_scale=2, rescale=False, crop=True):\n",
    "    ds = TiledHEDataset(img, context_img, context_scale)\n",
    "#     print(len(ds))\n",
    "#     raise RuntimeError()\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    if gene_idxs is None:\n",
    "        gene_idxs = np.arange(len(model.autokl.genes))\n",
    "#     print(len(ds))\n",
    "#     print(gene_idxs)\n",
    "    import psutil\n",
    "    print(psutil.virtual_memory())\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(dl))\n",
    "        he_x, he_context_x, top, left = batch['he'], batch['context_he'], batch['top'], batch['left']\n",
    "\n",
    "        if next(iter(model.autokl.parameters())).is_cuda:\n",
    "            he_x, he_context_x = he_x.cuda(), he_context_x.cuda()\n",
    "\n",
    "        result = model.autokl(he_x, he_context_x, reduce_to_tile=reduce_to_tile)\n",
    "        item = {k:v[0].detach().cpu()\n",
    "               for k, v in result.items()\n",
    "               if k in keep}\n",
    "        item['exp'] = item['exp'][:, :, gene_idxs]\n",
    "        if 'he' in item.keys():\n",
    "            item['he'] = rearrange(item['he'], 'c h w -> h w c')\n",
    "    max_r, max_c = ds.max_r(), ds.max_c()\n",
    "    print(max_r, max_c, window_scale)\n",
    "    for k, tile in item.items():\n",
    "        if k in keep:\n",
    "            print(tile.shape[0] * (max_r + 2) // window_scale, tile.shape[1] * (max_c + 2) // window_scale, tile.shape[2], tile.dtype)\n",
    "    print(psutil.virtual_memory())\n",
    "    key_to_new = {k:torch.ones(tile.shape[0] * (max_r + 2) // window_scale,\n",
    "                               tile.shape[1] * (max_c + 2) // window_scale,\n",
    "                               tile.shape[2],\n",
    "                               dtype=tile.dtype)\n",
    "                 for k, tile in item.items()\n",
    "                 if k in keep}\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dl:\n",
    "            he_x, he_context_x, top, left = batch['he'], batch['context_he'], batch['top'], batch['left']\n",
    "\n",
    "            if next(iter(model.autokl.parameters())).is_cuda:\n",
    "                he_x, he_context_x = he_x.cuda(), he_context_x.cuda()\n",
    "\n",
    "            result = model.autokl(he_x, he_context_x, reduce_to_tile=reduce_to_tile)\n",
    "\n",
    "            for i in range(result['exp'].shape[0]):\n",
    "                item = {k:v[i].detach().cpu()\n",
    "                       for k, v in result.items()\n",
    "                       if k in keep}\n",
    "                item['exp'] = item['exp'][:, :, gene_idxs]\n",
    "                if 'he' in item.keys():\n",
    "                    item['he'] = rearrange(item['he'], 'c h w -> h w c')\n",
    "                r, c = top[i], left[i]\n",
    "                for k in keep:\n",
    "                    tile = item[k]\n",
    "                    add_tile(tile, key_to_new[k], r, c, max_r, max_c, window_scale=window_scale)\n",
    "         \n",
    "    \n",
    "    if rescale:\n",
    "        for k, new in key_to_new.items():\n",
    "            if k == 'he':\n",
    "                new -= new.min()\n",
    "                new /= new.max()\n",
    "            else:\n",
    "                new -= new.amin(dim=(0, 1))\n",
    "                new /= new.amax(dim=(0, 1))\n",
    "\n",
    "    return key_to_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa201875",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8307104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = TiledHEDataset(train_adata.uns['rescaled_he']['2X_trimmed'],\n",
    "#                     train_adata.uns['rescaled_he']['8X_trimmed'],\n",
    "#                     4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8648f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_genes = ['EPCAM', 'KRT18', 'CD8A', 'IL7R', 'MS4A1', 'PECAM1', 'BGN']\n",
    "show_genes = ['KRT18', 'IL7R', 'PECAM1', 'BGN', 'INS']\n",
    "# show_genes = model.autokl.genes\n",
    "gene_idxs = [i for i, g in enumerate(model.autokl.genes) if g in show_genes]\n",
    "show_genes = model.autokl.genes[gene_idxs]\n",
    "key_to_retiled = predict_he(he, context_he, 4, model,\n",
    "                            gene_idxs=gene_idxs, rescale=True, window_scale=2,\n",
    "                            keep=('exp',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5083aa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, img in key_to_retiled.items():\n",
    "    print(k, img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d77c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(key_to_retiled['he'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1c3b12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, g in enumerate(show_genes):\n",
    "    plt.imshow(key_to_retiled['exp'][:, :, i])\n",
    "    plt.title(g)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f798948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(key_to_retiled['metagene_activity'].shape[-1]):\n",
    "    plt.imshow(key_to_retiled['metagene_activity'][:, :, i])\n",
    "    plt.title(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb51f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb3678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_to_retiled = predict_he(he, context_he, 4, model, gene_idxs=gene_idxs,\n",
    "                            reduce_to_tile=False, batch_size=2, rescale=True, window_scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db45af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, g in enumerate(show_genes):\n",
    "    plt.imshow(np.log1p(key_to_retiled['exp'][:, :, i]))\n",
    "    plt.title(g)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7093922",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cr1, cr2 = 3000, 4000\n",
    "cc1, cc2 = 3000, 4000\n",
    "for i, g in enumerate(show_genes):\n",
    "    plt.imshow(key_to_retiled['exp'][cr1:cr2, cc1:cc2, i])\n",
    "    plt.title(g)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9695015",
   "metadata": {},
   "outputs": [],
   "source": [
    "cr1, cr2 = 3000, 4000\n",
    "cc1, cc2 = 3000, 4000\n",
    "for i in range(key_to_retiled['metagene_activity'].shape[-1]):\n",
    "    plt.imshow(key_to_retiled['metagene_activity'][cr1:cr2, cc1:cc2, i])\n",
    "    plt.title(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c185d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(he[cr1:cr2, cc1:cc2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a99ccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_mask = a.uns['rescaled_spot_masks']['2X_trimmed']\n",
    "spot_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de71219",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(spot_mask[cr1:cr2, cc1:cc2]>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de981dba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7146211",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save gene expression image\n",
    "exp = TF.convert_image_dtype(key_to_retiled['exp'][:he.shape[0], :he.shape[1]], torch.uint8)\n",
    "exp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7cb9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(exp[:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c535f5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(exp, '../data/annotations/pdac/HT270P1-S1H1U1_exp_8x8.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17c751d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff58951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save gene expression image\n",
    "meta = TF.convert_image_dtype(key_to_retiled['metagene_activity'][:he.shape[0], :he.shape[1]], torch.uint8)\n",
    "meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b811d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(meta[:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9604dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(meta, '../data/annotations/pdac/HT270P1-S1H1U1_meta_8x8.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dd3989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0199fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2333ee78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddd29fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29287f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9281da77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "metagenes = model.autokl.metagenes.detach().cpu().numpy()\n",
    "metagenes = pd.DataFrame(data=metagenes, columns=model.autokl.genes, index=np.arange(metagenes.shape[0]))\n",
    "metagenes = metagenes.loc[:, show_genes]\n",
    "sns.clustermap(data=metagenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eb4e22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56511ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aec214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee864e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d2937c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2525f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "388ae9cb",
   "metadata": {},
   "source": [
    "#### apply directly to H&E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ff14c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
